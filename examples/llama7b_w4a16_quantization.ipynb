{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing Llama 7B to W4A16 Using SparseML's OneShot Pathway\n",
    "\n",
    "This example notebook walks through how to quantize Llama 7B using SparseML. We apply int4 channel-wise quantization all Linear layers, using UltraChat 200k as a calibration dataset.\n",
    "\n",
    "This example requires at least 30GB of GPU memory to run. The memory requirement can be reduced to 16GB by setting `sequential_update: true` in the recipe definition, but this will increase the runtime significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sparseml.transformers import SparseAutoModelForCausalLM, oneshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparseML uses recipes to define configurations for different oneshot algorithms. Recipes can be defined as a string or a yaml file. Below we create a sample recipe for GPTQ quantization. The recipe is made up of two different algorithms, called modifiers.\n",
    "\n",
    "1. **vLLMQuantizationModifier**: calibrates the model for quantization by calculating scale and zero points from a small amount of calibration data\n",
    "2. **SparseGPTModifier**: applies the GPTQ algorithm, using the result of the vLLMQuantizationModifier to determine the best quantization bin to place each linear weight into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe=\"\"\"\n",
    "test_stage:\n",
    "    quant_modifiers:\n",
    "        vLLMQuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    weights:\n",
    "                        num_bits: 4\n",
    "                        type: \"int\"\n",
    "                        symmetric: false\n",
    "                        strategy: \"channel\"\n",
    "                    targets: [\"Linear\"]\n",
    "        SparseGPTModifier:\n",
    "            sparsity: 0.0\n",
    "            quantize: True\n",
    "            sequential_update: true\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to initialize the model we wish to quantize, and define a dataset for calibration. We will use a llama2 7b model that has been pretrained on the ultrachat 200k dataset. We will use the same dataset the model has been pretrained on for our one shot calibration. \n",
    "\n",
    "SparseML supports several datasets, such as ultrachat-200k, out of the box. You can also pass in a tokenized `datasets.Dataset` object for custom dataset support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# by setting the device_map to auto, we can spread the model evenly across all available GPUs\n",
    "# load the model in as bfloat16 to save on memory and compute\n",
    "model_stub = \"zoo:llama2-7b-ultrachat200k_llama2_pretrain-base\"\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(model_stub, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# uses SparseML's built-in preprocessing for ultra chat\n",
    "dataset = \"ultrachat-200k\"\n",
    "\n",
    "# save location of quantized model\n",
    "output_dir = \"./output_llama7b_W4A16_channel_compressed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will configure our calibration dataset. To save on load time, we load only a small subset of ultrachat200k's `train_gen` split and label it as calibration data. For oneshot we do not need to pad the input, so we set `pad_to_max_length` to false. We also truncate each sample to a maximum of 512 tokens and select 512 samples for calibration. \n",
    "\n",
    "Using more calibration samples can improve model performance but will take longer to run. Generally 256-2048 calibration samples is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset config parameters\n",
    "splits = {\"calibration\": \"train_gen[:5%]\"}\n",
    "max_seq_length = 512\n",
    "pad_to_max_length = False\n",
    "num_calibration_samples = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can launch our quantization recipe using the `oneshot` function. This function call will apply the algorithms defined in `recipe` to the input `model`, using `num_calibration_samples` from `dataset` as calibration data. We will save the quantized model to `output_dir`.\n",
    "\n",
    "By setting `save_compressed` to True, the model will be saved with all of the quantized weights stored as int8, enabling the model to be loaded by vLLM. Once a model has been saved in this way, you can no longer reload the model in SparseML. To save the model in a \"fake quantized\" state instead so that it can be reloaded in SparseML or Transformers, set `save_compressed` to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    recipe=recipe,\n",
    "    output_dir=output_dir,\n",
    "    splits=splits,\n",
    "    max_seq_length=max_seq_length,\n",
    "    pad_to_max_length=pad_to_max_length,\n",
    "    num_calibration_samples=num_calibration_samples,\n",
    "    save_compressed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantized model should now be stored in the defined `output_dir`. Its `config.json` will contain a new `compression_config` field that describes how the model has been quantized. This config will be used to load the model into vLLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
