{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>&copy; 2021-present Neuralmagic, Inc. // [Neural Magic Legal](https://neuralmagic.com/legal)</sub> \n",
    "\n",
    "# TensorFlow v1 Classification Model Pruning Using SparseML\n",
    "\n",
    "This notebook provides a step-by-step walkthrough for pruning an already trained (dense) model to enable better performance at inference time using the DeepSparse Engine. You will:\n",
    "- Set up the model and dataset\n",
    "- Define a TensorFlow training flow with a simple SparseML integration\n",
    "- Prune the model using the TensorFlow+SparseML flow\n",
    "- Export to [ONNX](https://onnx.ai/)\n",
    "\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for how to plug SparseML into your TensorFlow training flow. Rough time estimates for fully pruning the default model are given. Note that training with the PyTorch CPU implementation will be much slower than a GPU:\n",
    "- 20 minutes on a GPU\n",
    "- 60 minutes on a laptop CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Requirements\n",
    "To run this notebook, you will need the following packages already installed:\n",
    "* SparseML and SparseZoo\n",
    "* TensorFlow v1 and tf2onnx\n",
    "\n",
    "You can install any package that is not already present via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparseml\n",
    "import sparsezoo\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "\n",
    "assert tf.__version__ < \"2\"\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Setting Up the Model and Dataset\n",
    "\n",
    "By default, you will prune a [ResNet-50](https://arxiv.org/abs/1512.03385) model trained on the [Imagenette dataset](https://github.com/fastai/imagenette). The model's pretrained weights are downloaded from the SparseZoo model repo.   The Imagenette dataset is downloaded from its repository via a helper class from SparseML.\n",
    "\n",
    "In the cells below, functions are defined to load the dataset, model, and training objects to be called during training from within the `Graph` context.\n",
    "\n",
    "If you would like to try out your model for pruning, modify the appropriate function for to load your model or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from sparseml.tensorflow_v1.models import ModelRegistry\n",
    "from sparseml.tensorflow_v1.datasets import (\n",
    "    ImagenetteDataset,\n",
    "    ImagenetteSize,\n",
    ")\n",
    "from sparseml.tensorflow_v1.utils import (\n",
    "    batch_cross_entropy_loss,\n",
    "    accuracy,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"resnet50\"\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = 224\n",
    "NUM_CLASSES = 10\n",
    "SAVE_DIR = \"tensorflow_v1_classification_pruning\"\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        print(\"loading datasets\")\n",
    "        train_dataset = ImagenetteDataset(\n",
    "            train=True, dataset_size=ImagenetteSize.s320, image_size=INPUT_SIZE\n",
    "        )\n",
    "        train_len = len(train_dataset)\n",
    "        train_steps = math.ceil(train_len / float(BATCH_SIZE))\n",
    "        train_dataset = train_dataset.build(\n",
    "            BATCH_SIZE,\n",
    "            shuffle_buffer_size=1000,\n",
    "            prefetch_buffer_size=BATCH_SIZE,\n",
    "            num_parallel_calls=4,\n",
    "        )\n",
    "\n",
    "        val_dataset = ImagenetteDataset(\n",
    "            train=False, dataset_size=ImagenetteSize.s320, image_size=INPUT_SIZE\n",
    "        )\n",
    "        val_len = len(val_dataset)\n",
    "        val_steps = math.ceil(val_len / float(BATCH_SIZE))\n",
    "        val_dataset = val_dataset.build(\n",
    "            BATCH_SIZE,\n",
    "            shuffle_buffer_size=1000,\n",
    "            prefetch_buffer_size=BATCH_SIZE,\n",
    "            num_parallel_calls=4,\n",
    "        )\n",
    "\n",
    "    return train_dataset, val_dataset, (train_steps, val_steps)\n",
    "\n",
    "\n",
    "def create_model(sample_input, training):\n",
    "    print(\"Creating model graph for {}\".format(MODEL_NAME))\n",
    "    logits = ModelRegistry.create(\n",
    "        MODEL_NAME,\n",
    "        inputs=sample_input,\n",
    "        training=training,\n",
    "        num_classes=NUM_CLASSES,\n",
    "    )\n",
    "    return logits\n",
    "\n",
    "\n",
    "def create_training_objects(sample_labels):\n",
    "    print(\"Creating loss, accuracy, and optimizer in graph\")\n",
    "    loss = batch_cross_entropy_loss(logits, labels)\n",
    "    acc = accuracy(logits, labels)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.00008).minimize(\n",
    "        loss, global_step=global_step\n",
    "    )\n",
    "    return loss, acc, global_step, train_op\n",
    "\n",
    "\n",
    "def load_pretrained():\n",
    "    print(\"loading pre-trained model weights\")\n",
    "    ModelRegistry.load_pretrained(\n",
    "        MODEL_NAME, pretrained=\"base\", remove_dynamic_tl_vars=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create a SparseML Modifier Manager\n",
    "\n",
    "To prune a model with SparseML, you will download a recipe from SparseZoo and use it to create a `ScheduledModifierManager` object.  This manager will be used to create operations that modify the training process.\n",
    "\n",
    "You can create SparseML recipes to perform various model pruning schedules, quantization aware training, sparse transfer learning, and more.  If you are using a different model than the default, you will have to modify the recipe YAML file to target the new model's parameters.\n",
    "\n",
    "Using the operators generated from this manager object, you will be able to prune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparseml.tensorflow_v1.optim import (\n",
    "    ScheduledModifierManager,\n",
    ")\n",
    "from sparsezoo import Zoo\n",
    "\n",
    "\n",
    "def create_sparseml_manager():\n",
    "    recipe = Zoo.search_recipes(\n",
    "        domain=\"cv\",\n",
    "        sub_domain=\"classification\",\n",
    "        architecture=\"resnet_v1\",\n",
    "        sub_architecture=\"50\",\n",
    "        framework=\"tensorflow_v1\",\n",
    "        repo=\"sparseml\",\n",
    "        dataset=\"imagenette\",\n",
    "        sparse_name=\"pruned\",\n",
    "    )[0]  # unwrap search result\n",
    "    recipe.download()\n",
    "    \n",
    "    recipe_path = recipe.downloaded_path()\n",
    "    print(recipe)\n",
    "    print(f\"Recipe downloaded to: {recipe_path}\")\n",
    "\n",
    "    return ScheduledModifierManager.from_yaml(recipe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Prune your model using a TensorFlow training loop\n",
    "SparseML can plug directly into your existing TensorFlow training flow by creating additional operators to run. To demonstrate this, in the cell below, prune the model using a standard TensorFlow training loop while also running the operators created by the manager object.  To prune your existing models using SparseML, you can use your own training flow with the additional operators added.\n",
    "\n",
    "For your convienence the lines needed for integrating with SparseML are preceeded by large comment blocks.\n",
    "\n",
    "If the kernel shuts down during training, this may be an out of memory error, to resolve this, try lowering the `BATCH_SIZE` in Step 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading datasets\n",
      "already downloaded imagenette ImagenetteSize.s320\n",
      "already downloaded imagenette ImagenetteSize.s320\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/src/sparseml/tensorflow_v1/datasets/helpers.py:131: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /tmp/ipykernel_863114/2057596615.py:16: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Creating model graph for resnet50\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/src/sparseml/tensorflow_v1/nn/layers.py:217: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/tf/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/src/sparseml/tensorflow_v1/nn/layers.py:229: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/src/sparseml/tensorflow_v1/nn/layers.py:130: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From /home/konstantin/Source/sparseml/src/sparseml/tensorflow_v1/nn/layers.py:394: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "Creating loss, accuracy, and optimizer in graph\n",
      "WARNING:tensorflow:From /tmp/ipykernel_863114/834035773.py:65: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_863114/834035773.py:66: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_863114/2057596615.py:19: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_863114/2057596615.py:19: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "Recipe downloaded to: /home/konstantin/.cache/sparsezoo/e971390a-ffbc-4ac7-b18e-54bf7bec36d2/recipes/original.yaml\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'log_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_863114/2057596615.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# create sparseml training manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sparseml_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmod_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_extras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_863114/4078616071.py\u001b[0m in \u001b[0;36mcreate_sparseml_manager\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recipe downloaded to: {recipe_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mScheduledModifierManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Source/sparseml/src/sparseml/tensorflow_v1/optim/manager.py\u001b[0m in \u001b[0;36mfrom_yaml\u001b[0;34m(file_path, add_modifiers, recipe_variables)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mrecipe_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_recipe_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0myaml_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_recipe_yaml_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrecipe_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mmodifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_modifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmodifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_modifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/src/sparseml/tensorflow_v1/optim/modifier.py\u001b[0m in \u001b[0;36mload_list\u001b[0;34m(yaml_str)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mmodifiers\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mModifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_framework_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTENSORFLOW_V1_FRAMEWORK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/src/sparseml/optim/modifier.py\u001b[0m in \u001b[0;36mload_framework_list\u001b[0;34m(yaml_str, framework)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0myaml_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_recipe_yaml_str_equations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0myaml_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseModifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_framework_modifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/tf/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msafe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muntrusted\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSafeLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/tf/lib/python3.7/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/tf/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/tf/lib/python3.7/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_generators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdummy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/sparseml/src/sparseml/optim/modifier.py\u001b[0m in \u001b[0;36mconstructor\u001b[0;34m(loader, node)\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'log_types'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from sparseml.utils import create_unique_dir, create_dirs\n",
    "from sparseml.tensorflow_v1.datasets import create_split_iterators_handle\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    # create dataset\n",
    "    train_dataset, val_dataset, (train_steps, val_steps) = load_dataset()\n",
    "    handle, iterator, (train_iter, val_iter) = create_split_iterators_handle(\n",
    "        [train_dataset, val_dataset]\n",
    "    )\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    # create base training objects\n",
    "    training = tf.placeholder(dtype=tf.bool, shape=[])\n",
    "    logits = create_model(images, training)\n",
    "    loss, acc, global_step, train_op = create_training_objects(labels)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    #######################################################\n",
    "    # create sparseml training manager\n",
    "    #######################################################\n",
    "    manager = create_sparseml_manager()\n",
    "    mod_ops, mod_extras = manager.create_ops(train_steps, global_step, graph=graph)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"initializing session\")\n",
    "        sess.run(\n",
    "            [\n",
    "                tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer(),\n",
    "            ]\n",
    "        )\n",
    "        train_iter_handle, val_iter_handle = sess.run(\n",
    "            [train_iter.string_handle(), val_iter.string_handle()]\n",
    "        )\n",
    "\n",
    "        # initialize sparseml manager after pretrained weights loaded\n",
    "        load_pretrained()\n",
    "        manager.initialize_session()\n",
    "\n",
    "        num_epochs = manager.max_epochs\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"pruning\"):\n",
    "            print(\"training for epoch {}...\".format(epoch))\n",
    "            sess.run(train_iter.initializer)\n",
    "            train_losses = []\n",
    "            train_acc = []\n",
    "\n",
    "            for step in range(train_steps):\n",
    "                _, __, meas_step, meas_loss, meas_acc = sess.run(\n",
    "                    [train_op, update_ops, global_step, loss, acc],\n",
    "                    feed_dict={handle: train_iter_handle, training: True},\n",
    "                )\n",
    "                train_losses.append(meas_loss)\n",
    "                train_acc.append(meas_acc)\n",
    "\n",
    "                #######################################################\n",
    "                # Modifier update ops line for transfer learning from a sparse model in TensorFlow\n",
    "                #######################################################\n",
    "                sess.run(mod_ops)\n",
    "            print(\n",
    "                \"completed epoch {} training with: loss {} / acc {}\".format(\n",
    "                    epoch,\n",
    "                    numpy.mean(train_losses).item(),\n",
    "                    numpy.mean(train_acc).item() * 100,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(\"validating for epoch {}...\".format(epoch))\n",
    "            sess.run(val_iter.initializer)\n",
    "            val_losses = []\n",
    "            val_acc = []\n",
    "\n",
    "            for step in range(val_steps):\n",
    "                meas_loss, meas_acc = sess.run(\n",
    "                    [loss, acc],\n",
    "                    feed_dict={handle: val_iter_handle, training: False},\n",
    "                )\n",
    "                val_losses.append(meas_loss)\n",
    "                val_acc.append(meas_acc)\n",
    "\n",
    "            print(\n",
    "                \"completed epoch {} validation with: loss {} / acc {}\".format(\n",
    "                    epoch,\n",
    "                    numpy.mean(val_losses).item(),\n",
    "                    numpy.mean(val_acc).item() * 100,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        #######################################################\n",
    "        # Final line for sparseml training in TensorFlow, complete the graph\n",
    "        #######################################################\n",
    "        manager.complete_graph()\n",
    "\n",
    "        NAME = \"resnet50-imagenette-pruned\"\n",
    "        checkpoint_path = create_unique_dir(\n",
    "            os.path.join(\".\", SAVE_DIR, NAME, \"checkpoint\")\n",
    "        )\n",
    "        checkpoint_path = os.path.join(checkpoint_path, \"model\")\n",
    "        create_dirs(checkpoint_path)\n",
    "        saver = ModelRegistry.saver(MODEL_NAME)\n",
    "        saver.save(sess, checkpoint_path)\n",
    "        print(\"saved model checkpoint to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Exporting to ONNX\n",
    "\n",
    "Now that the model is fully recalibrated, you need to export it to an ONNX format, which is the format used by the DeepSparse Engine. For TensorFlow, exporting to ONNX is not natively supported. To add support, you will use the `tf2onnx` Python package. In the cell block below, a convenience class, `GraphExporter()`, is used to handle exporting. It wraps the somewhat complicated API for `tf2onnx` into an easy to use interface.\n",
    "\n",
    "Note, for some configurations, the tf2onnx code does not work properly in a Jupyter Notebook. To remedy this, you should run the `exporter.export_onnx()` function call in a Python console or script.\n",
    "\n",
    "Once the model is saved as an ONNX ﬁle, it is ready to be used for inference with the DeepSparse Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparseml.utils import clean_path\n",
    "from sparseml.tensorflow_v1.utils import GraphExporter\n",
    "\n",
    "\n",
    "export_path = clean_path(os.path.join(\".\", SAVE_DIR, NAME))\n",
    "exporter = GraphExporter(export_path)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    print(\"Recreating graph...\", flush=True)\n",
    "\n",
    "    input_placeholder = tf.placeholder(\n",
    "        tf.float32, [None, INPUT_SIZE, INPUT_SIZE, 3], name=\"inputs\"\n",
    "    )\n",
    "    logits = create_model(input_placeholder, training=False)\n",
    "\n",
    "    input_names = [input_placeholder.name]\n",
    "    output_names = [logits.name]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Restoring previous weights...\", flush=True)\n",
    "        saver = ModelRegistry.saver(MODEL_NAME)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "\n",
    "        print(\"Exporting to pb...\", flush=True)\n",
    "        exporter.export_pb(outputs=[logits])\n",
    "        print(\"Exported pb file to {}\".format(exporter.pb_path), flush=True)\n",
    "\n",
    "print(\"Exporting to onnx...\", flush=True)\n",
    "exporter.export_onnx(inputs=input_names, outputs=output_names)\n",
    "print(\"Exported onnx file to {}\".format(exporter.onnx_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations, you have pruned a model and exported it to ONNX for inference!  Next steps you can pursue include:\n",
    "* Pruning different models using SparseML\n",
    "* Trying different pruning and optimization recipes\n",
    "* Running your model on the DeepSparse Engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
