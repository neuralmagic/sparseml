{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivkTnb7u7x03"
      },
      "source": [
        "\n",
        "A notebook using Neural Magic's [SparseML](https://github.com/neuralmagic/sparseml) library to convert a dense Hugging Face model into a light and super fast sparsified model! This notebook has the potential to unlock 1000's of dense models previously fine-tuned and uploaded on Hugging Face's Model Hub. üöÄüöÄüöÄ\n",
        "\n",
        "<br>\n",
        "\n",
        "To learn more about sparse transfer learning, check out the docs [here](https://docs.neuralmagic.com/get-started/transfer-a-sparsified-model/nlp-text-classification).\n",
        "\n",
        "<br>\n",
        "\n",
        "This notebook allows devs to:\n",
        "*   Download a dense BERT base uncased model from the Hugging Face Model Hub.\n",
        "*   Distill this dense model (teacher) onto a sparse pre-trained transformer (sparse student) by using the [emotion dataset](https://huggingface.co/datasets/emotion).\n",
        "*   Export our model to ONNX format.\n",
        "*   Run inference using the super fast DeepSparse Engine.\n",
        "\n",
        "There only a few requirements necessary to allow for the sparse-transfer of select Hugging Face models:\n",
        "*   Does SparseML currently support the NLP task?\n",
        "*   Does SparseML currently support the model architecture?\n",
        "*   Is the dataset of interest available for download/training?\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "In the example below, we'll sparse-transfer a dense BERT base uncased onto a 6 layered pruned quantized BERT from the Neural Magic [SparseZoo](https://sparsezoo.neuralmagic.com/?domain=nlp&sub_domain=masked_language_modeling&page=1). We'll use a [dense BERT](https://huggingface.co/nateraw/bert-base-uncased-emotion?text=I+like+you.+I+love+you) previously fine-tuned on the emotion dataset, which is a multi-class classification task.\n",
        "\n",
        "Emotion is a dataset of English Twitter messages with six basic emotions: `anger`, `fear`, `joy`, `love`, `sadness`, and `surprise`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKVIy2aiE7EW"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi # double check you're in a GPU runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DecOUdgJ9WFh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYtFOA4KONX"
      },
      "source": [
        "Now let's download a dense BERT base uncased that was previously fine-tuned on the emotion dataset and save the tokenizer and weights to a folder called `dense model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6th0-l18H2D"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
        "tokenizer.save_pretrained(\"/content/dense_model\")\n",
        "model.save_pretrained(\"/content/dense_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1XocoqFzCU"
      },
      "source": [
        "Let's now install the sparseml library to begin the sparse transfer from the dense model to a sparse model. After install, you may need to `restart runtime`, and re-run the cell block to make sure dependencies were installed correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIpfXokUBlKZ"
      },
      "outputs": [],
      "source": [
        "!pip install sparseml[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGlWL__nGqb7"
      },
      "source": [
        "Run the following the CLI command to initiate the sparse transfer learning.The dense model, as seen in the `distill_teacher` argument, will transfer its knowledge onto a 6 layer pruned quantized bert base student model, as seen in the `model_name_or_path` argument. When doing sparse transferring, your dense model's architecture needs to match the architecture of the student model, which in this case is bert base uncased.\n",
        "\n",
        "The modifiers required to do this transfer can be found in the `recipe`. To learn mmore about recipes and its modifiers, you can read more in the [docs](https://docs.neuralmagic.com/user-guide/recipes).\n",
        "\n",
        "Unlike the training parameters of traditional fine-tuning, the parameters when conducting sparse-transfer learning are a lot more sensitive. And a good heuristic to start with is learning to calibrate the most critical parameters during the training: the `initial learning rate` and `number of epochs`. These parameters are hard coded in the recipe, however, to speed things up for this example, we've already tinkered with various values for these two parameters and have overridden the recipe with custom values found in the `recipe_args` argument. Most likely, for any model you do sparse transfer learning, these values will be overriden during your tinkering process.\n",
        "\n",
        "The following command will give you a sparse model with an accuracy close to ~92.5% on the validation dataset with a total training time of ~47 mins with a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LNgQ2Rv9k5d"
      },
      "outputs": [],
      "source": [
        "!sparseml.transformers.train.text_classification \\\n",
        "  --output_dir sparse_model \\\n",
        "\t--model_name_or_path zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/6layer_pruned80_quant-none-vnni \\\n",
        "\t--distill_teacher ./dense_model/ \\\n",
        "\t--recipe zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni?recipe_type=transfer-text_classification \\\n",
        "  --dataset_name \"emotion\" \\\n",
        "  --recipe_args '{\"num_epochs\":9, \"init_lr\":0.000057}' \\\n",
        "\t--do_train \\\n",
        "\t--do_eval \\\n",
        "  --eval_steps 200 \\\n",
        "\t--max_seq_length 128 \\\n",
        "\t--evaluation_strategy steps \\\n",
        "\t--per_device_train_batch_size 32 \\\n",
        "\t--per_device_eval_batch_size 32 \\\n",
        "\t--preprocessing_num_workers 8 \\\n",
        "\t--fp16 \\\n",
        "\t--seed 42 \\\n",
        "\t--save_strategy steps \\\n",
        "\t--save_steps 200 \\\n",
        "\t--save_total_limit 3 \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--load_best_model_at_end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkq_oRzwHVT5"
      },
      "source": [
        "Now that we have a sparse model, we can now export its PyTorch weights into ONNX format with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nF9eeA2HfPA"
      },
      "outputs": [],
      "source": [
        "!sparseml.transformers.export_onnx --model_path sparse_model --task 'text_classification' --sequence_length 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M9SUhDGhvOW"
      },
      "source": [
        "Le'ts do the same to the dense model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reCoVSJOhutS"
      },
      "outputs": [],
      "source": [
        "!sparseml.transformers.export_onnx --model_path dense_model --task 'text_classification' --sequence_length 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npvhmyj5Hfog"
      },
      "source": [
        "Let's now install [DeepSparse](https://github.com/neuralmagic/deepsparse) and benchmark these two models on the colab's single CPU and compare their speeds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOaKwCT9Hgj4"
      },
      "outputs": [],
      "source": [
        "!pip install deepsparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9CpG1uVauQx"
      },
      "outputs": [],
      "source": [
        "!deepsparse.benchmark dense_model/model.onnx --batch_size 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu84qy9Tileq"
      },
      "outputs": [],
      "source": [
        "!deepsparse.benchmark sparse_model/model.onnx --batch_size 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eojmQ3IyjqoG"
      },
      "source": [
        "Pretty incredible, the dense model gives us a latency of 378 ms while the new sparse model is gives us a latency of 46 ms, an 8X speedup!! ü§Øü§Øü§Ø\n",
        "\n",
        "<br>\n",
        "\n",
        "For more resources, you can always give [SparseML](https://github.com/neuralmagic/sparseml) and [DeepSparse](https://github.com/neuralmagic/deepsparse) a ‚≠ê, and let us know what you think on our [slack community channel](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
