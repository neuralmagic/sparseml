{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivkTnb7u7x03"
      },
      "source": [
        "# Sparse Transfer Hugging Face NLP Models using SparseML!\n",
        "\n",
        "This notebook uses Neural Magic's [SparseML](https://github.com/neuralmagic/sparseml) library to convert a dense Hugging Face model into a light and super fast sparsified model! This in turn has the potential to unlock 1000's of dense models previously fine-tuned and uploaded onto the Hugging Face Models Hub. 🚀🚀🚀\n",
        "\n",
        "<br>\n",
        "\n",
        "To learn more about sparse transfer learning, check out the docs [here](https://docs.neuralmagic.com/get-started/transfer-a-sparsified-model/nlp-text-classification).\n",
        "\n",
        "<br>\n",
        "\n",
        "This notebook allows devs to:\n",
        "*   Install the SparseML library for sparse-transfer training.\n",
        "*   Distill a dense model (teacher) onto a sparse pre-trained transformer (sparse student).\n",
        "*   Export the sparse and dense models to ONNX format.\n",
        "*   Benchmark the dense and sparse models using DeepSparse.\n",
        "\n",
        "To know the eligibility for which Hugging Face models are able to be sparse-transferred, SparseML must:\n",
        "*   Currently support the NLP task.\n",
        "*   Currently support the model architecture.\n",
        "*   Have the dataset of interest available for download.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "In the example below, we'll sparse-transfer a dense BERT base uncased onto a pruned quantized oBERT from the Neural Magic [SparseZoo](https://sparsezoo.neuralmagic.com/?domain=nlp&sub_domain=masked_language_modeling&page=1). We'll use a [dense BERT](https://huggingface.co/nateraw/bert-base-uncased-emotion?text=I+like+you.+I+love+you) previously fine-tuned on the emotion dataset, which is a multi-class classification task, as our teacher model.\n",
        "\n",
        "The [Emotion dataset](https://huggingface.co/datasets/emotion) consists of English Twitter messages with six basic emotions: `anger`, `fear`, `joy`, `love`, `sadness`, and `surprise`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKVIy2aiE7EW",
        "outputId": "e3d5f74c-1a15-4816-ad15-33272924fb76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Nov 17 15:35:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # double check you're in a GPU runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gsXMeB-ETTB"
      },
      "source": [
        "Download the SparseML library to get access to the Transformers library fork and the required version of PyTorch to do our training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIpfXokUBlKZ",
        "outputId": "e25ce73a-b2ff-47ff-db4d-0cbac2932232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sparseml[torch]\n",
            "  Downloading sparseml-1.2.0-py3-none-any.whl (827 kB)\n",
            "\u001b[K     |████████████████████████████████| 827 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting onnx<=1.10.1,>=1.5.0\n",
            "  Downloading onnx-1.10.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.7.3)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (3.19.6)\n",
            "Collecting GPUtil>=1.4.0\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (3.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (21.3)\n",
            "Collecting click~=8.0.0\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools<=59.5.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (57.4.0)\n",
            "Requirement already satisfied: pydantic>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.0.2)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (7.7.1)\n",
            "Collecting merge-args>=0.1.0\n",
            "  Downloading merge_args-0.1.4-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: pyyaml>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (5.4.8)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (2.23.0)\n",
            "Collecting jupyter>=1.0.0\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: progressbar2>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (3.38.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (0.18.3)\n",
            "Collecting sparsezoo~=1.2.0\n",
            "  Downloading sparsezoo-1.2.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting toposort>=1.0\n",
            "  Downloading toposort-1.7-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting tensorboardX>=1.0\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting gputils\n",
            "  Downloading gputils-1.0.6-py3-none-any.whl (3.8 kB)\n",
            "Collecting torch<1.9.2,>=1.1.0\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 6.5 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.9,>=1.0\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click~=8.0.0->sparseml[torch]) (4.13.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (3.0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (3.6.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (6.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.8.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (6.1.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.7.16)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->sparseml[torch]) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx<=1.10.1,>=1.5.0->sparseml[torch]) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->sparseml[torch]) (2022.6)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.0.0->sparseml[torch]) (3.4.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (2.10)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2.9.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->sparseml[torch]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->sparseml[torch]) (3.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (2.14.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (1.50.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (0.38.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=1.0->sparseml[torch]) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=1.0->sparseml[torch]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=1.0->sparseml[torch]) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=1.0->sparseml[torch]) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=1.0->sparseml[torch]) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click~=8.0.0->sparseml[torch]) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=1.0->sparseml[torch]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=1.0->sparseml[torch]) (3.2.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (0.13.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (4.11.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (23.2.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (0.15.0)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (2.11.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (5.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (1.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook->jupyter>=1.0.0->sparseml[torch]) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (5.0.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (0.19.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0.0->sparseml[torch]) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.5.1)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7410 sha256=e0e3204d9d38a7632438c6a88db6efa333641dd5a89a89b2ada12ceab0fb1d23\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: jedi, qtpy, qtconsole, onnx, click, toposort, sparsezoo, merge-args, jupyter, GPUtil, torch, tensorboardX, tensorboard, sparseml, gputils\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.1 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.8.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.4 which is incompatible.\u001b[0m\n",
            "Successfully installed GPUtil-1.4.0 click-8.0.4 gputils-1.0.6 jedi-0.18.1 jupyter-1.0.0 merge-args-0.1.4 onnx-1.10.1 qtconsole-5.4.0 qtpy-2.3.0 sparseml-1.2.0 sparsezoo-1.2.0 tensorboard-2.8.0 tensorboardX-2.5.1 toposort-1.7 torch-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sparseml[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGlWL__nGqb7"
      },
      "source": [
        "Run the following the CLI command to initiate the sparse transfer learning. The dense model, as seen in the `distill_teacher` argument, will transfer its knowledge onto a 6 layer pruned quantized bert base student model, as seen in the `model_name_or_path` argument.\n",
        "\n",
        "The modifiers required to do this transfer can be found in the `recipe`. To learn mmore about recipes and its modifiers, you can read more in the [docs](https://docs.neuralmagic.com/user-guide/recipes).\n",
        "\n",
        "Unlike the training parameters of traditional fine-tuning, the parameters when conducting sparse-transfer learning are a lot more sensitive. And a good heuristic to start with is learning to calibrate the most critical parameters during the training: the `initial learning rate` and `number of epochs`. These parameters are hard coded in the recipe, however, to speed things up for this example, we've already tinkered with various values for these two parameters and have overridden the recipe with custom values found in the `recipe_args` argument. Most likely, for any model you do sparse transfer learning, these values will be overriden during your tinkering process.\n",
        "\n",
        "The following command will give you a sparse model with an accuracy close to ~92.5% on the validation dataset with a total training/evaluation time of ~55 mins with a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LNgQ2Rv9k5d",
        "outputId": "ab58b3b9-43f1-4d63-8842-8a1bc04eb576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6807489395141602, max_val=1.498458743095398)\n",
            "        )\n",
            "      )\n",
            "      (token_type_embeddings): Embedding(\n",
            "        2, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2120935916900635, max_val=1.9974937438964844)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([6], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.217111587524414, max_val=1.9924442768096924)\n",
            "        )\n",
            "      )\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3796744346618652, max_val=1.4984676837921143)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9384058713912964, max_val=3.281951904296875)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.723798930644989, max_val=0.5648977756500244)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0609], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.554149627685547, max_val=7.984903812408447)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0966], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.413786888122559, max_val=12.320796966552734)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9188], device='cuda:0'), zero_point=tensor([138], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-264.2058410644531, max_val=225.100341796875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905548334121704)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0363], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.626159191131592, max_val=4.157615661621094)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0261], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.562986135482788, max_val=3.1000208854675293)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0252], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4335055351257324, max_val=2.9966444969177246)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7911482453346252, max_val=0.9569889307022095)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([209], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.252166748046875, max_val=3.9957878589630127)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.19011926651001, max_val=3.3235294818878174)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9320], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=237.48097229003906)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0805], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.25825309753418, max_val=3.588470458984375)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): Identity()\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0090], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1424946784973145, max_val=1.0853772163391113)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4342875480651855, max_val=1.1374919414520264)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0056], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6622740626335144, max_val=0.7114232778549194)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0655], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.382139205932617, max_val=8.327349662780762)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1280], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.640097618103027, max_val=16.317705154418945)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1766], device='cuda:0'), zero_point=tensor([165], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-358.57958984375, max_val=196.45240783691406)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905717611312866)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0397], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.301239967346191, max_val=5.0589470863342285)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0232], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0078978538513184, max_val=2.8967480659484863)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.891711473464966, max_val=2.7827415466308594)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6539136171340942, max_val=0.879724383354187)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8450], device='cuda:0'), zero_point=tensor([244], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-206.58990478515625, max_val=8.880552291870117)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.742603302001953, max_val=5.154741287231445)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4656], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=118.56425476074219)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2265], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-28.88271713256836, max_val=2.546034812927246)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): Identity()\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1166847944259644, max_val=1.0752073526382446)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3314846754074097, max_val=1.5024046897888184)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6636859774589539, max_val=0.5986616611480713)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0744], device='cuda:0'), zero_point=tensor([122], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.056354522705078, max_val=9.918448448181152)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1786], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.276399612426758, max_val=22.775074005126953)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4415], device='cuda:0'), zero_point=tensor([107], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-369.8003845214844, max_val=507.7763366699219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.090548038482666)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.839170455932617, max_val=4.811429023742676)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0197], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5742313861846924, max_val=2.4605908393859863)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0189], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.469463348388672, max_val=2.356276750564575)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9055518507957458, max_val=0.7723783254623413)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3018], device='cuda:0'), zero_point=tensor([230], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-69.2968521118164, max_val=7.654778003692627)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0209], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.344242572784424, max_val=2.6698484420776367)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2668], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=67.8621826171875)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.969645500183105, max_val=4.398077487945557)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): Identity()\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.104039192199707, max_val=1.0940190553665161)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2823474407196045, max_val=1.0981290340423584)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7351449131965637, max_val=0.7537226676940918)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0985], device='cuda:0'), zero_point=tensor([120], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.86684513092041, max_val=13.250986099243164)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1684], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.24740219116211, max_val=21.47601318359375)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.4523], device='cuda:0'), zero_point=tensor([101], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-549.301025390625, max_val=841.0320434570312)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0904561281204224)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0484], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.176854133605957, max_val=5.174277305603027)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0243], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0886051654815674, max_val=3.1156625747680664)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9666240215301514, max_val=2.997638702392578)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4109056293964386, max_val=0.5645580291748047)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2428], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.25087356567383, max_val=11.655906677246094)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.885575771331787, max_val=2.102328300476074)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1498], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=38.01658630371094)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.834123373031616, max_val=1.2167776823043823)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): Identity()\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.092060923576355, max_val=0.9687017202377319)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1083917617797852, max_val=1.091858983039856)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7720397710800171, max_val=0.7179197072982788)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0948], device='cuda:0'), zero_point=tensor([122], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.589058876037598, max_val=12.595871925354004)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1997], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-25.4593563079834, max_val=24.01837730407715)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.6603], device='cuda:0'), zero_point=tensor([94], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-625.1889038085938, max_val=1073.1756591796875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905715227127075)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0460], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.863100051879883, max_val=5.744691371917725)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.897671699523926, max_val=5.162110328674316)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.71154260635376, max_val=4.965884208679199)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6395687460899353, max_val=0.5617991089820862)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2268], device='cuda:0'), zero_point=tensor([225], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-51.12342071533203, max_val=6.705932140350342)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7547035217285156, max_val=1.132000207901001)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0649], device='cuda:0'), zero_point=tensor([3], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=16.37074089050293)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.848039627075195, max_val=1.8184455633163452)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): Identity()\n",
            "        (10): Identity()\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2490218877792358, max_val=1.2170183658599854)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5546650886535645, max_val=1.382936954498291)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6350619196891785, max_val=0.6284002065658569)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0894], device='cuda:0'), zero_point=tensor([118], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.542417526245117, max_val=12.262940406799316)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2983], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-37.27627182006836, max_val=38.037864685058594)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([20.2475], device='cuda:0'), zero_point=tensor([242], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4892.46337890625, max_val=270.6487731933594)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905404090881348)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0469], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.817821025848389, max_val=5.975724220275879)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0399], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.1015496253967285, max_val=5.065960884094238)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.896567344665527, max_val=4.890985488891602)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7808759808540344, max_val=0.8295480608940125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1116], device='cuda:0'), zero_point=tensor([206], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-23.00933837890625, max_val=5.4444451332092285)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8427028656005859, max_val=0.7472058534622192)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0239], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=5.936534404754639)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5402920246124268, max_val=1.0929075479507446)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): QuantWrapper(\n",
            "        (quant): QuantStub(\n",
            "          (activation_post_process): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([145], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.439950466156006, max_val=4.919440269470215)\n",
            "          )\n",
            "        )\n",
            "        (dequant): DeQuantStub()\n",
            "        (module): Linear(\n",
            "          in_features=768, out_features=768, bias=True\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0008], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09760898351669312, max_val=0.09701567143201828)\n",
            "          )\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "      )\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): QuantWrapper(\n",
            "    (quant): QuantStub(\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0085], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0886750221252441, max_val=1.0884482860565186)\n",
            "      )\n",
            "    )\n",
            "    (dequant): DeQuantStub()\n",
            "    (module): Linear(\n",
            "      in_features=768, out_features=6, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0006], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07268338650465012, max_val=0.07974009215831757)\n",
            "      )\n",
            "      (activation_post_process): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "INFO:sparseml.transformers.sparsification.trainer:BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(\n",
            "        30522, 768, padding_idx=0\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0093], device='cuda:0'), zero_point=tensor([156], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4561666250228882, max_val=0.9219279885292053)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([34], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.405395984649658, max_val=1.3836086988449097)\n",
            "        )\n",
            "      )\n",
            "      (position_embeddings): Embedding(\n",
            "        512, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([135], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.683095932006836, max_val=1.4960863590240479)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6807489395141602, max_val=1.498458743095398)\n",
            "        )\n",
            "      )\n",
            "      (token_type_embeddings): Embedding(\n",
            "        2, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2120935916900635, max_val=1.9974937438964844)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([6], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.217111587524414, max_val=1.9924442768096924)\n",
            "        )\n",
            "      )\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3796744346618652, max_val=1.4984676837921143)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9384058713912964, max_val=3.281951904296875)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.691760540008545, max_val=2.639540195465088)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.723798930644989, max_val=0.5648977756500244)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0609], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.554149627685547, max_val=7.984903812408447)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0966], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.413786888122559, max_val=12.320796966552734)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9188], device='cuda:0'), zero_point=tensor([138], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-264.2058410644531, max_val=225.100341796875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905548334121704)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0363], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.626159191131592, max_val=4.157615661621094)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0261], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.562986135482788, max_val=3.1000208854675293)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0252], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4335055351257324, max_val=2.9966444969177246)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7911482453346252, max_val=0.9569889307022095)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([209], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.252166748046875, max_val=3.9957878589630127)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.19011926651001, max_val=3.3235294818878174)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9320], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=237.48097229003906)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0805], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.25825309753418, max_val=3.588470458984375)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): Identity()\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0090], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1424946784973145, max_val=1.0853772163391113)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4342875480651855, max_val=1.1374919414520264)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0631], device='cuda:0'), zero_point=tensor([176], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098682403564453, max_val=4.983479022979736)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0056], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6622740626335144, max_val=0.7114232778549194)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0655], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.382139205932617, max_val=8.327349662780762)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1280], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.640097618103027, max_val=16.317705154418945)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1766], device='cuda:0'), zero_point=tensor([165], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-358.57958984375, max_val=196.45240783691406)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905717611312866)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0397], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.301239967346191, max_val=5.0589470863342285)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0232], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0078978538513184, max_val=2.8967480659484863)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.891711473464966, max_val=2.7827415466308594)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6539136171340942, max_val=0.879724383354187)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8450], device='cuda:0'), zero_point=tensor([244], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-206.58990478515625, max_val=8.880552291870117)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.742603302001953, max_val=5.154741287231445)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4656], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=118.56425476074219)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2265], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-28.88271713256836, max_val=2.546034812927246)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): Identity()\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1166847944259644, max_val=1.0752073526382446)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3314846754074097, max_val=1.5024046897888184)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([161], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.932180404663086, max_val=4.619839191436768)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6636859774589539, max_val=0.5986616611480713)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0744], device='cuda:0'), zero_point=tensor([122], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.056354522705078, max_val=9.918448448181152)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1786], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.276399612426758, max_val=22.775074005126953)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4415], device='cuda:0'), zero_point=tensor([107], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-369.8003845214844, max_val=507.7763366699219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.090548038482666)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.839170455932617, max_val=4.811429023742676)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0197], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5742313861846924, max_val=2.4605908393859863)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0189], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.469463348388672, max_val=2.356276750564575)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9055518507957458, max_val=0.7723783254623413)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3018], device='cuda:0'), zero_point=tensor([230], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-69.2968521118164, max_val=7.654778003692627)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0209], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.344242572784424, max_val=2.6698484420776367)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2668], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=67.8621826171875)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.969645500183105, max_val=4.398077487945557)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): Identity()\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.104039192199707, max_val=1.0940190553665161)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2823474407196045, max_val=1.0981290340423584)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([150], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.947739601135254, max_val=11.242683410644531)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7351449131965637, max_val=0.7537226676940918)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0985], device='cuda:0'), zero_point=tensor([120], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.86684513092041, max_val=13.250986099243164)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1684], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.24740219116211, max_val=21.47601318359375)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.4523], device='cuda:0'), zero_point=tensor([101], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-549.301025390625, max_val=841.0320434570312)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0904561281204224)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0484], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.176854133605957, max_val=5.174277305603027)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0243], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0886051654815674, max_val=3.1156625747680664)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9666240215301514, max_val=2.997638702392578)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4109056293964386, max_val=0.5645580291748047)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2428], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.25087356567383, max_val=11.655906677246094)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.885575771331787, max_val=2.102328300476074)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1498], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=38.01658630371094)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.834123373031616, max_val=1.2167776823043823)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): Identity()\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.092060923576355, max_val=0.9687017202377319)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1083917617797852, max_val=1.091858983039856)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1234], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.733610153198242, max_val=12.743762969970703)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7720397710800171, max_val=0.7179197072982788)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0948], device='cuda:0'), zero_point=tensor([122], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.589058876037598, max_val=12.595871925354004)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1997], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-25.4593563079834, max_val=24.01837730407715)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.6603], device='cuda:0'), zero_point=tensor([94], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-625.1889038085938, max_val=1073.1756591796875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905715227127075)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0460], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.863100051879883, max_val=5.744691371917725)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.897671699523926, max_val=5.162110328674316)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.71154260635376, max_val=4.965884208679199)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6395687460899353, max_val=0.5617991089820862)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2268], device='cuda:0'), zero_point=tensor([225], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-51.12342071533203, max_val=6.705932140350342)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7547035217285156, max_val=1.132000207901001)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0649], device='cuda:0'), zero_point=tensor([3], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=16.37074089050293)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.848039627075195, max_val=1.8184455633163452)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): Identity()\n",
            "        (10): Identity()\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2490218877792358, max_val=1.2170183658599854)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5546650886535645, max_val=1.382936954498291)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1092], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.609655380249023, max_val=8.234783172607422)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6350619196891785, max_val=0.6284002065658569)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0894], device='cuda:0'), zero_point=tensor([118], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.542417526245117, max_val=12.262940406799316)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2983], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-37.27627182006836, max_val=38.037864685058594)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([20.2475], device='cuda:0'), zero_point=tensor([242], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4892.46337890625, max_val=270.6487731933594)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0905404090881348)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0469], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.817821025848389, max_val=5.975724220275879)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0399], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.1015496253967285, max_val=5.065960884094238)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.896567344665527, max_val=4.890985488891602)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7808759808540344, max_val=0.8295480608940125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1116], device='cuda:0'), zero_point=tensor([206], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-23.00933837890625, max_val=5.4444451332092285)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8427028656005859, max_val=0.7472058534622192)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0239], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=5.936534404754639)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5402920246124268, max_val=1.0929075479507446)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): QuantWrapper(\n",
            "        (quant): QuantStub(\n",
            "          (activation_post_process): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([145], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.439950466156006, max_val=4.919440269470215)\n",
            "          )\n",
            "        )\n",
            "        (dequant): DeQuantStub()\n",
            "        (module): Linear(\n",
            "          in_features=768, out_features=768, bias=True\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0008], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09760898351669312, max_val=0.09701567143201828)\n",
            "          )\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "      )\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): QuantWrapper(\n",
            "    (quant): QuantStub(\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0085], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0886750221252441, max_val=1.0884482860565186)\n",
            "      )\n",
            "    )\n",
            "    (dequant): DeQuantStub()\n",
            "    (module): Linear(\n",
            "      in_features=768, out_features=6, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0006], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07268338650465012, max_val=0.07974009215831757)\n",
            "      )\n",
            "      (activation_post_process): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            " 80% 3600/4500 [34:40<10:12,  1.47it/s][INFO|trainer.py:571] 2022-11-17 16:13:12,350 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:13:12,354 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:13:12,354 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:13:12,354 >>   Batch size = 32\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:12,  4.78it/s]\u001b[A\n",
            "  5% 3/63 [00:00<00:17,  3.38it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:20,  2.92it/s]\u001b[A\n",
            "  8% 5/63 [00:01<00:21,  2.71it/s]\u001b[A\n",
            " 10% 6/63 [00:02<00:22,  2.58it/s]\u001b[A\n",
            " 11% 7/63 [00:02<00:22,  2.51it/s]\u001b[A\n",
            " 13% 8/63 [00:02<00:22,  2.47it/s]\u001b[A\n",
            " 14% 9/63 [00:03<00:22,  2.44it/s]\u001b[A\n",
            " 16% 10/63 [00:03<00:21,  2.42it/s]\u001b[A\n",
            " 17% 11/63 [00:04<00:21,  2.40it/s]\u001b[A\n",
            " 19% 12/63 [00:04<00:21,  2.38it/s]\u001b[A\n",
            " 21% 13/63 [00:05<00:21,  2.37it/s]\u001b[A\n",
            " 22% 14/63 [00:05<00:21,  2.33it/s]\u001b[A\n",
            " 24% 15/63 [00:05<00:20,  2.30it/s]\u001b[A\n",
            " 25% 16/63 [00:06<00:20,  2.27it/s]\u001b[A\n",
            " 27% 17/63 [00:06<00:20,  2.27it/s]\u001b[A\n",
            " 29% 18/63 [00:07<00:19,  2.26it/s]\u001b[A\n",
            " 30% 19/63 [00:07<00:19,  2.23it/s]\u001b[A\n",
            " 32% 20/63 [00:08<00:19,  2.20it/s]\u001b[A\n",
            " 33% 21/63 [00:08<00:19,  2.18it/s]\u001b[A\n",
            " 35% 22/63 [00:09<00:18,  2.17it/s]\u001b[A\n",
            " 37% 23/63 [00:09<00:18,  2.15it/s]\u001b[A\n",
            " 38% 24/63 [00:10<00:18,  2.14it/s]\u001b[A\n",
            " 40% 25/63 [00:10<00:17,  2.12it/s]\u001b[A\n",
            " 41% 26/63 [00:11<00:16,  2.20it/s]\u001b[A\n",
            " 43% 27/63 [00:11<00:16,  2.24it/s]\u001b[A\n",
            " 44% 28/63 [00:11<00:15,  2.27it/s]\u001b[A\n",
            " 46% 29/63 [00:12<00:14,  2.29it/s]\u001b[A\n",
            " 48% 30/63 [00:12<00:14,  2.30it/s]\u001b[A\n",
            " 49% 31/63 [00:13<00:13,  2.33it/s]\u001b[A\n",
            " 51% 32/63 [00:13<00:13,  2.35it/s]\u001b[A\n",
            " 52% 33/63 [00:13<00:12,  2.35it/s]\u001b[A\n",
            " 54% 34/63 [00:14<00:12,  2.36it/s]\u001b[A\n",
            " 56% 35/63 [00:14<00:11,  2.35it/s]\u001b[A\n",
            " 57% 36/63 [00:15<00:11,  2.35it/s]\u001b[A\n",
            " 59% 37/63 [00:15<00:11,  2.34it/s]\u001b[A\n",
            " 60% 38/63 [00:16<00:10,  2.35it/s]\u001b[A\n",
            " 62% 39/63 [00:16<00:10,  2.35it/s]\u001b[A\n",
            " 63% 40/63 [00:16<00:09,  2.35it/s]\u001b[A\n",
            " 65% 41/63 [00:17<00:09,  2.35it/s]\u001b[A\n",
            " 67% 42/63 [00:17<00:08,  2.36it/s]\u001b[A\n",
            " 68% 43/63 [00:18<00:08,  2.37it/s]\u001b[A\n",
            " 70% 44/63 [00:18<00:08,  2.37it/s]\u001b[A\n",
            " 71% 45/63 [00:19<00:07,  2.37it/s]\u001b[A\n",
            " 73% 46/63 [00:19<00:07,  2.37it/s]\u001b[A\n",
            " 75% 47/63 [00:19<00:06,  2.37it/s]\u001b[A\n",
            " 76% 48/63 [00:20<00:06,  2.36it/s]\u001b[A\n",
            " 78% 49/63 [00:20<00:05,  2.36it/s]\u001b[A\n",
            " 79% 50/63 [00:21<00:05,  2.36it/s]\u001b[A\n",
            " 81% 51/63 [00:21<00:05,  2.36it/s]\u001b[A\n",
            " 83% 52/63 [00:22<00:04,  2.36it/s]\u001b[A\n",
            " 84% 53/63 [00:22<00:04,  2.35it/s]\u001b[A\n",
            " 86% 54/63 [00:22<00:03,  2.36it/s]\u001b[A\n",
            " 87% 55/63 [00:23<00:03,  2.37it/s]\u001b[A\n",
            " 89% 56/63 [00:23<00:02,  2.36it/s]\u001b[A\n",
            " 90% 57/63 [00:24<00:02,  2.37it/s]\u001b[A\n",
            " 92% 58/63 [00:24<00:02,  2.35it/s]\u001b[A\n",
            " 94% 59/63 [00:24<00:01,  2.35it/s]\u001b[A\n",
            " 95% 60/63 [00:25<00:01,  2.35it/s]\u001b[A\n",
            " 97% 61/63 [00:25<00:00,  2.35it/s]\u001b[A\n",
            " 98% 62/63 [00:26<00:00,  2.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.14405274391174316, 'eval_accuracy': 0.9225000143051147, 'eval_runtime': 26.97, 'eval_samples_per_second': 74.156, 'eval_steps_per_second': 2.336, 'epoch': 7.2}\n",
            " 80% 3600/4500 [35:07<10:12,  1.47it/s]\n",
            "100% 63/63 [00:26<00:00,  2.65it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2161] 2022-11-17 16:13:39,326 >> Saving model checkpoint to sparse_model/checkpoint-3600\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:13:39,327 >> Configuration saved in sparse_model/checkpoint-3600/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:13:40,027 >> Model weights saved in sparse_model/checkpoint-3600/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:13:40,028 >> tokenizer config file saved in sparse_model/checkpoint-3600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:13:40,028 >> Special tokens file saved in sparse_model/checkpoint-3600/special_tokens_map.json\n",
            "2022-11-17 16:13:40 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/checkpoint-3600/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/checkpoint-3600/recipe.yaml\n",
            "[INFO|trainer.py:2239] 2022-11-17 16:13:42,038 >> Deleting older checkpoint [sparse_model/checkpoint-3000] due to args.save_total_limit\n",
            " 84% 3800/4500 [37:28<07:59,  1.46it/s][INFO|trainer.py:571] 2022-11-17 16:16:00,057 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:16:00,062 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:16:00,062 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:16:00,062 >>   Batch size = 32\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:12,  4.80it/s]\u001b[A\n",
            "  5% 3/63 [00:00<00:17,  3.36it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:20,  2.89it/s]\u001b[A\n",
            "  8% 5/63 [00:01<00:21,  2.70it/s]\u001b[A\n",
            " 10% 6/63 [00:02<00:22,  2.58it/s]\u001b[A\n",
            " 11% 7/63 [00:02<00:22,  2.51it/s]\u001b[A\n",
            " 13% 8/63 [00:02<00:22,  2.47it/s]\u001b[A\n",
            " 14% 9/63 [00:03<00:22,  2.44it/s]\u001b[A\n",
            " 16% 10/63 [00:03<00:21,  2.42it/s]\u001b[A\n",
            " 17% 11/63 [00:04<00:21,  2.40it/s]\u001b[A\n",
            " 19% 12/63 [00:04<00:21,  2.39it/s]\u001b[A\n",
            " 21% 13/63 [00:05<00:20,  2.38it/s]\u001b[A\n",
            " 22% 14/63 [00:05<00:20,  2.37it/s]\u001b[A\n",
            " 24% 15/63 [00:05<00:20,  2.38it/s]\u001b[A\n",
            " 25% 16/63 [00:06<00:19,  2.38it/s]\u001b[A\n",
            " 27% 17/63 [00:06<00:19,  2.37it/s]\u001b[A\n",
            " 29% 18/63 [00:07<00:18,  2.37it/s]\u001b[A\n",
            " 30% 19/63 [00:07<00:18,  2.36it/s]\u001b[A\n",
            " 32% 20/63 [00:08<00:18,  2.36it/s]\u001b[A\n",
            " 33% 21/63 [00:08<00:17,  2.36it/s]\u001b[A\n",
            " 35% 22/63 [00:08<00:17,  2.35it/s]\u001b[A\n",
            " 37% 23/63 [00:09<00:16,  2.37it/s]\u001b[A\n",
            " 38% 24/63 [00:09<00:16,  2.37it/s]\u001b[A\n",
            " 40% 25/63 [00:10<00:16,  2.37it/s]\u001b[A\n",
            " 41% 26/63 [00:10<00:15,  2.38it/s]\u001b[A\n",
            " 43% 27/63 [00:10<00:15,  2.38it/s]\u001b[A\n",
            " 44% 28/63 [00:11<00:14,  2.38it/s]\u001b[A\n",
            " 46% 29/63 [00:11<00:14,  2.37it/s]\u001b[A\n",
            " 48% 30/63 [00:12<00:13,  2.37it/s]\u001b[A\n",
            " 49% 31/63 [00:12<00:13,  2.36it/s]\u001b[A\n",
            " 51% 32/63 [00:13<00:13,  2.38it/s]\u001b[A\n",
            " 52% 33/63 [00:13<00:12,  2.37it/s]\u001b[A\n",
            " 54% 34/63 [00:13<00:12,  2.38it/s]\u001b[A\n",
            " 56% 35/63 [00:14<00:11,  2.37it/s]\u001b[A\n",
            " 57% 36/63 [00:14<00:11,  2.36it/s]\u001b[A\n",
            " 59% 37/63 [00:15<00:10,  2.38it/s]\u001b[A\n",
            " 60% 38/63 [00:15<00:10,  2.38it/s]\u001b[A\n",
            " 62% 39/63 [00:16<00:10,  2.38it/s]\u001b[A\n",
            " 63% 40/63 [00:16<00:09,  2.37it/s]\u001b[A\n",
            " 65% 41/63 [00:16<00:09,  2.36it/s]\u001b[A\n",
            " 67% 42/63 [00:17<00:08,  2.36it/s]\u001b[A\n",
            " 68% 43/63 [00:17<00:08,  2.36it/s]\u001b[A\n",
            " 70% 44/63 [00:18<00:08,  2.36it/s]\u001b[A\n",
            " 71% 45/63 [00:18<00:07,  2.37it/s]\u001b[A\n",
            " 73% 46/63 [00:18<00:07,  2.37it/s]\u001b[A\n",
            " 75% 47/63 [00:19<00:06,  2.36it/s]\u001b[A\n",
            " 76% 48/63 [00:19<00:06,  2.36it/s]\u001b[A\n",
            " 78% 49/63 [00:20<00:05,  2.36it/s]\u001b[A\n",
            " 79% 50/63 [00:20<00:05,  2.35it/s]\u001b[A\n",
            " 81% 51/63 [00:21<00:05,  2.36it/s]\u001b[A\n",
            " 83% 52/63 [00:21<00:04,  2.36it/s]\u001b[A\n",
            " 84% 53/63 [00:21<00:04,  2.35it/s]\u001b[A\n",
            " 86% 54/63 [00:22<00:03,  2.36it/s]\u001b[A\n",
            " 87% 55/63 [00:22<00:03,  2.35it/s]\u001b[A\n",
            " 89% 56/63 [00:23<00:02,  2.35it/s]\u001b[A\n",
            " 90% 57/63 [00:23<00:02,  2.35it/s]\u001b[A\n",
            " 92% 58/63 [00:24<00:02,  2.35it/s]\u001b[A\n",
            " 94% 59/63 [00:24<00:01,  2.37it/s]\u001b[A\n",
            " 95% 60/63 [00:24<00:01,  2.37it/s]\u001b[A\n",
            " 97% 61/63 [00:25<00:00,  2.37it/s]\u001b[A\n",
            " 98% 62/63 [00:25<00:00,  2.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.13458071649074554, 'eval_accuracy': 0.9225000143051147, 'eval_runtime': 26.4848, 'eval_samples_per_second': 75.515, 'eval_steps_per_second': 2.379, 'epoch': 7.6}\n",
            " 84% 3800/4500 [37:54<07:59,  1.46it/s]\n",
            "100% 63/63 [00:26<00:00,  2.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2161] 2022-11-17 16:16:26,548 >> Saving model checkpoint to sparse_model/checkpoint-3800\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:16:26,550 >> Configuration saved in sparse_model/checkpoint-3800/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:16:27,298 >> Model weights saved in sparse_model/checkpoint-3800/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:16:27,299 >> tokenizer config file saved in sparse_model/checkpoint-3800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:16:27,299 >> Special tokens file saved in sparse_model/checkpoint-3800/special_tokens_map.json\n",
            "2022-11-17 16:16:27 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/checkpoint-3800/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/checkpoint-3800/recipe.yaml\n",
            "[INFO|trainer.py:2239] 2022-11-17 16:16:29,262 >> Deleting older checkpoint [sparse_model/checkpoint-3200] due to args.save_total_limit\n",
            "{'loss': 0.1085, 'learning_rate': 6.3459999999999965e-06, 'epoch': 8.0}\n",
            " 89% 4000/4500 [40:15<05:40,  1.47it/s][INFO|trainer.py:571] 2022-11-17 16:18:46,868 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:18:46,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:18:46,872 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:18:46,872 >>   Batch size = 32\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:12,  4.83it/s]\u001b[A\n",
            "  5% 3/63 [00:00<00:17,  3.36it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:20,  2.92it/s]\u001b[A\n",
            "  8% 5/63 [00:01<00:21,  2.69it/s]\u001b[A\n",
            " 10% 6/63 [00:02<00:22,  2.57it/s]\u001b[A\n",
            " 11% 7/63 [00:02<00:22,  2.50it/s]\u001b[A\n",
            " 13% 8/63 [00:02<00:22,  2.46it/s]\u001b[A\n",
            " 14% 9/63 [00:03<00:22,  2.41it/s]\u001b[A\n",
            " 16% 10/63 [00:03<00:22,  2.40it/s]\u001b[A\n",
            " 17% 11/63 [00:04<00:21,  2.38it/s]\u001b[A\n",
            " 19% 12/63 [00:04<00:21,  2.37it/s]\u001b[A\n",
            " 21% 13/63 [00:05<00:21,  2.37it/s]\u001b[A\n",
            " 22% 14/63 [00:05<00:20,  2.36it/s]\u001b[A\n",
            " 24% 15/63 [00:05<00:20,  2.37it/s]\u001b[A\n",
            " 25% 16/63 [00:06<00:19,  2.37it/s]\u001b[A\n",
            " 27% 17/63 [00:06<00:19,  2.37it/s]\u001b[A\n",
            " 29% 18/63 [00:07<00:18,  2.37it/s]\u001b[A\n",
            " 30% 19/63 [00:07<00:18,  2.36it/s]\u001b[A\n",
            " 32% 20/63 [00:08<00:18,  2.36it/s]\u001b[A\n",
            " 33% 21/63 [00:08<00:17,  2.36it/s]\u001b[A\n",
            " 35% 22/63 [00:08<00:17,  2.36it/s]\u001b[A\n",
            " 37% 23/63 [00:09<00:16,  2.37it/s]\u001b[A\n",
            " 38% 24/63 [00:09<00:16,  2.35it/s]\u001b[A\n",
            " 40% 25/63 [00:10<00:16,  2.37it/s]\u001b[A\n",
            " 41% 26/63 [00:10<00:15,  2.36it/s]\u001b[A\n",
            " 43% 27/63 [00:11<00:15,  2.36it/s]\u001b[A\n",
            " 44% 28/63 [00:11<00:14,  2.36it/s]\u001b[A\n",
            " 46% 29/63 [00:11<00:14,  2.36it/s]\u001b[A\n",
            " 48% 30/63 [00:12<00:13,  2.36it/s]\u001b[A\n",
            " 49% 31/63 [00:12<00:13,  2.36it/s]\u001b[A\n",
            " 51% 32/63 [00:13<00:13,  2.35it/s]\u001b[A\n",
            " 52% 33/63 [00:13<00:12,  2.35it/s]\u001b[A\n",
            " 54% 34/63 [00:13<00:12,  2.35it/s]\u001b[A\n",
            " 56% 35/63 [00:14<00:11,  2.35it/s]\u001b[A\n",
            " 57% 36/63 [00:14<00:11,  2.35it/s]\u001b[A\n",
            " 59% 37/63 [00:15<00:11,  2.35it/s]\u001b[A\n",
            " 60% 38/63 [00:15<00:10,  2.35it/s]\u001b[A\n",
            " 62% 39/63 [00:16<00:10,  2.35it/s]\u001b[A\n",
            " 63% 40/63 [00:16<00:09,  2.36it/s]\u001b[A\n",
            " 65% 41/63 [00:16<00:09,  2.36it/s]\u001b[A\n",
            " 67% 42/63 [00:17<00:08,  2.36it/s]\u001b[A\n",
            " 68% 43/63 [00:17<00:08,  2.36it/s]\u001b[A\n",
            " 70% 44/63 [00:18<00:08,  2.36it/s]\u001b[A\n",
            " 71% 45/63 [00:18<00:07,  2.35it/s]\u001b[A\n",
            " 73% 46/63 [00:19<00:07,  2.36it/s]\u001b[A\n",
            " 75% 47/63 [00:19<00:06,  2.36it/s]\u001b[A\n",
            " 76% 48/63 [00:19<00:06,  2.36it/s]\u001b[A\n",
            " 78% 49/63 [00:20<00:05,  2.37it/s]\u001b[A\n",
            " 79% 50/63 [00:20<00:05,  2.36it/s]\u001b[A\n",
            " 81% 51/63 [00:21<00:05,  2.37it/s]\u001b[A\n",
            " 83% 52/63 [00:21<00:04,  2.37it/s]\u001b[A\n",
            " 84% 53/63 [00:22<00:04,  2.36it/s]\u001b[A\n",
            " 86% 54/63 [00:22<00:03,  2.37it/s]\u001b[A\n",
            " 87% 55/63 [00:22<00:03,  2.36it/s]\u001b[A\n",
            " 89% 56/63 [00:23<00:02,  2.36it/s]\u001b[A\n",
            " 90% 57/63 [00:23<00:02,  2.36it/s]\u001b[A\n",
            " 92% 58/63 [00:24<00:02,  2.36it/s]\u001b[A\n",
            " 94% 59/63 [00:24<00:01,  2.35it/s]\u001b[A\n",
            " 95% 60/63 [00:25<00:01,  2.34it/s]\u001b[A\n",
            " 97% 61/63 [00:25<00:00,  2.35it/s]\u001b[A\n",
            " 98% 62/63 [00:25<00:00,  2.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.1321648508310318, 'eval_accuracy': 0.9259999990463257, 'eval_runtime': 26.6078, 'eval_samples_per_second': 75.166, 'eval_steps_per_second': 2.368, 'epoch': 8.0}\n",
            " 89% 4000/4500 [40:41<05:40,  1.47it/s]\n",
            "100% 63/63 [00:26<00:00,  2.55it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2161] 2022-11-17 16:19:13,481 >> Saving model checkpoint to sparse_model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:19:13,486 >> Configuration saved in sparse_model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:19:14,484 >> Model weights saved in sparse_model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:19:14,485 >> tokenizer config file saved in sparse_model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:19:14,485 >> Special tokens file saved in sparse_model/checkpoint-4000/special_tokens_map.json\n",
            "2022-11-17 16:19:14 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/checkpoint-4000/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/checkpoint-4000/recipe.yaml\n",
            "[INFO|trainer.py:2239] 2022-11-17 16:19:16,335 >> Deleting older checkpoint [sparse_model/checkpoint-3400] due to args.save_total_limit\n",
            "2022-11-17 16:19:16 sparseml.transformers.sparsification.trainer INFO     BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(\n",
            "        30522, 768, padding_idx=0\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0093], device='cuda:0'), zero_point=tensor([156], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4561666250228882, max_val=0.9216761589050293)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([34], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.405395984649658, max_val=1.3836086988449097)\n",
            "        )\n",
            "      )\n",
            "      (position_embeddings): Embedding(\n",
            "        512, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([135], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.68308687210083, max_val=1.496076226234436)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6809747219085693, max_val=1.4981482028961182)\n",
            "        )\n",
            "      )\n",
            "      (token_type_embeddings): Embedding(\n",
            "        2, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.212069511413574, max_val=1.997464895248413)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([6], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.217212200164795, max_val=1.9922116994857788)\n",
            "        )\n",
            "      )\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3794304132461548, max_val=1.4984676837921143)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9386708736419678, max_val=3.281951904296875)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239922881126404, max_val=0.564814031124115)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([125], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.439045429229736, max_val=7.706223487854004)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0944], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.230441093444824, max_val=12.041165351867676)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8704], device='cuda:0'), zero_point=tensor([140], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-261.20452880859375, max_val=215.7407684326172)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549750328063965)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.455430030822754, max_val=4.101684093475342)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0249], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3799946308135986, max_val=2.9702627658843994)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0244], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3239660263061523, max_val=2.8955469131469727)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7910841107368469, max_val=0.9569889307022095)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0857], device='cuda:0'), zero_point=tensor([210], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.98406982421875, max_val=3.8665683269500732)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.190065383911133, max_val=3.323415756225586)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9475], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=241.43173217773438)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0805], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.257736206054688, max_val=3.588524103164673)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): Identity()\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0090], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1422111988067627, max_val=1.0848069190979004)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4339810609817505, max_val=1.1376045942306519)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0056], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6624572277069092, max_val=0.7114232778549194)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0643], device='cuda:0'), zero_point=tensor([129], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.263673782348633, max_val=8.133392333984375)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1261], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.500116348266602, max_val=16.07465934753418)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1477], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-352.0048522949219, max_val=195.66766357421875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549980401992798)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.301650047302246, max_val=4.9206013679504395)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([129], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.929433822631836, max_val=2.845444917678833)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8565244674682617, max_val=2.755784034729004)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6537507176399231, max_val=0.879724383354187)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8368], device='cuda:0'), zero_point=tensor([247], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-206.41358947753906, max_val=6.964414119720459)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.742908477783203, max_val=5.154741287231445)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3682], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=93.72615814208984)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2265], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-28.88249969482422, max_val=2.546034812927246)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): Identity()\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.116195559501648, max_val=1.0753200054168701)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.331120252609253, max_val=1.5024046897888184)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6640061736106873, max_val=0.5984693765640259)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0699], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.632515907287598, max_val=9.193807601928711)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1781], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.274944305419922, max_val=22.710100173950195)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.3110], device='cuda:0'), zero_point=tensor([106], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-352.23529052734375, max_val=492.0595397949219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.054942011833191)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.914254188537598, max_val=4.8112311363220215)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([132], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.557568073272705, max_val=2.393528461456299)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187], device='cuda:0'), zero_point=tensor([131], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.4538657665252686, max_val=2.321977138519287)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9054067730903625, max_val=0.7721974849700928)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2969], device='cuda:0'), zero_point=tensor([230], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-68.37860870361328, max_val=7.323338031768799)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0209], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3441810607910156, max_val=2.6698484420776367)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2594], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=65.96587371826172)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.969585418701172, max_val=4.397980690002441)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): Identity()\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1040866374969482, max_val=1.0939151048660278)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2820971012115479, max_val=1.0979859828948975)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.735069215297699, max_val=0.7537226676940918)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0935], device='cuda:0'), zero_point=tensor([120], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.231294631958008, max_val=12.622262954711914)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1672], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.042747497558594, max_val=21.31846046447754)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.3305], device='cuda:0'), zero_point=tensor([101], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-538.3626708984375, max_val=820.9024047851562)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549033880233765)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.09459114074707, max_val=5.118311882019043)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0235], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9835121631622314, max_val=3.0016465187072754)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0229], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.90596079826355, max_val=2.924959182739258)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41061997413635254, max_val=0.5645580291748047)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2426], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.29859161376953, max_val=11.560476303100586)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8858323097229004, max_val=2.1025872230529785)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1240], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=31.44314956665039)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.834296226501465, max_val=1.2163833379745483)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): Identity()\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0919495820999146, max_val=0.9687157869338989)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1087861061096191, max_val=1.0919421911239624)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7721608877182007, max_val=0.7179678082466125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0918], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.278984069824219, max_val=12.137971878051758)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1995], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-25.44156837463379, max_val=24.005590438842773)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.1611], device='cuda:0'), zero_point=tensor([97], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-594.733642578125, max_val=976.3569946289062)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549964904785156)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.826887607574463, max_val=5.745780944824219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.799777030944824, max_val=5.050131320953369)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0376], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.666715621948242, max_val=4.912500858306885)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6389373540878296, max_val=0.5619038343429565)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2267], device='cuda:0'), zero_point=tensor([226], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-51.21174621582031, max_val=6.595755100250244)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7547343969345093, max_val=1.132254958152771)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0584], device='cuda:0'), zero_point=tensor([3], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=14.731457710266113)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.847890853881836, max_val=1.818128228187561)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): Identity()\n",
            "        (10): Identity()\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.249541997909546, max_val=1.2171351909637451)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5546003580093384, max_val=1.382991909980774)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6351149082183838, max_val=0.6283748149871826)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0844], device='cuda:0'), zero_point=tensor([121], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.183473587036133, max_val=11.342721939086914)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2967], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-36.96077346801758, max_val=37.82801055908203)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([20.1340], device='cuda:0'), zero_point=tensor([242], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4870.40966796875, max_val=263.76953125)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549681186676025)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0465], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.77514123916626, max_val=5.931796073913574)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0397], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.048597812652588, max_val=5.084214210510254)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.8913421630859375, max_val=4.933834075927734)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7808341979980469, max_val=0.8295480608940125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1086], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.42236328125, max_val=5.262651443481445)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.842707633972168, max_val=0.7473668456077576)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0240], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=5.961795806884766)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.540349245071411, max_val=1.0925571918487549)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): QuantWrapper(\n",
            "        (quant): QuantStub(\n",
            "          (activation_post_process): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([146], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.598691463470459, max_val=4.8956828117370605)\n",
            "          )\n",
            "        )\n",
            "        (dequant): DeQuantStub()\n",
            "        (module): Linear(\n",
            "          in_features=768, out_features=768, bias=True\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0008], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09762699902057648, max_val=0.09738196432590485)\n",
            "          )\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "      )\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): QuantWrapper(\n",
            "    (quant): QuantStub(\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0083], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0535449981689453, max_val=1.0532469749450684)\n",
            "      )\n",
            "    )\n",
            "    (dequant): DeQuantStub()\n",
            "    (module): Linear(\n",
            "      in_features=768, out_features=6, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0006], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07281223684549332, max_val=0.07974009215831757)\n",
            "      )\n",
            "      (activation_post_process): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "INFO:sparseml.transformers.sparsification.trainer:BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(\n",
            "        30522, 768, padding_idx=0\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0093], device='cuda:0'), zero_point=tensor([156], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4561666250228882, max_val=0.9216761589050293)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([34], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.405395984649658, max_val=1.3836086988449097)\n",
            "        )\n",
            "      )\n",
            "      (position_embeddings): Embedding(\n",
            "        512, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([135], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.68308687210083, max_val=1.496076226234436)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6809747219085693, max_val=1.4981482028961182)\n",
            "        )\n",
            "      )\n",
            "      (token_type_embeddings): Embedding(\n",
            "        2, 768\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.212069511413574, max_val=1.997464895248413)\n",
            "        )\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([6], device='cuda:0')\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.217212200164795, max_val=1.9922116994857788)\n",
            "        )\n",
            "      )\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3794304132461548, max_val=1.4984676837921143)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9386708736419678, max_val=3.281951904296875)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([183], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.473200798034668, max_val=2.5540730953216553)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239922881126404, max_val=0.564814031124115)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([125], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.439045429229736, max_val=7.706223487854004)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0944], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.230441093444824, max_val=12.041165351867676)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8704], device='cuda:0'), zero_point=tensor([140], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-261.20452880859375, max_val=215.7407684326172)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549750328063965)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.455430030822754, max_val=4.101684093475342)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0249], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3799946308135986, max_val=2.9702627658843994)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0244], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3239660263061523, max_val=2.8955469131469727)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0075], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7910841107368469, max_val=0.9569889307022095)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0857], device='cuda:0'), zero_point=tensor([210], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.98406982421875, max_val=3.8665683269500732)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.190065383911133, max_val=3.323415756225586)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9475], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=241.43173217773438)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0805], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.257736206054688, max_val=3.588524103164673)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): Identity()\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0090], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1422111988067627, max_val=1.0848069190979004)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4339810609817505, max_val=1.1376045942306519)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([186], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.064311981201172, max_val=4.090519905090332)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0056], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6624572277069092, max_val=0.7114232778549194)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0643], device='cuda:0'), zero_point=tensor([129], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.263673782348633, max_val=8.133392333984375)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1261], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.500116348266602, max_val=16.07465934753418)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1477], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-352.0048522949219, max_val=195.66766357421875)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549980401992798)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.301650047302246, max_val=4.9206013679504395)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([129], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.929433822631836, max_val=2.845444917678833)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([130], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8565244674682617, max_val=2.755784034729004)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6537507176399231, max_val=0.879724383354187)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8368], device='cuda:0'), zero_point=tensor([247], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-206.41358947753906, max_val=6.964414119720459)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.742908477783203, max_val=5.154741287231445)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3682], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=93.72615814208984)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2265], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-28.88249969482422, max_val=2.546034812927246)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): Identity()\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.116195559501648, max_val=1.0753200054168701)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.331120252609253, max_val=1.5024046897888184)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([164], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.520436763763428, max_val=4.1453351974487305)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6640061736106873, max_val=0.5984693765640259)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0699], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.632515907287598, max_val=9.193807601928711)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1781], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.274944305419922, max_val=22.710100173950195)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.3110], device='cuda:0'), zero_point=tensor([106], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-352.23529052734375, max_val=492.0595397949219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.054942011833191)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.914254188537598, max_val=4.8112311363220215)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([132], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.557568073272705, max_val=2.393528461456299)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187], device='cuda:0'), zero_point=tensor([131], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.4538657665252686, max_val=2.321977138519287)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9054067730903625, max_val=0.7721974849700928)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2969], device='cuda:0'), zero_point=tensor([230], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-68.37860870361328, max_val=7.323338031768799)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0209], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3441810607910156, max_val=2.6698484420776367)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2594], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=65.96587371826172)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.969585418701172, max_val=4.397980690002441)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): Identity()\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1040866374969482, max_val=1.0939151048660278)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2820971012115479, max_val=1.0979859828948975)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.414894104003906, max_val=11.084259033203125)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.735069215297699, max_val=0.7537226676940918)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0935], device='cuda:0'), zero_point=tensor([120], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.231294631958008, max_val=12.622262954711914)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1672], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.042747497558594, max_val=21.31846046447754)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.3305], device='cuda:0'), zero_point=tensor([101], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-538.3626708984375, max_val=820.9024047851562)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549033880233765)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.09459114074707, max_val=5.118311882019043)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0235], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9835121631622314, max_val=3.0016465187072754)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0229], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.90596079826355, max_val=2.924959182739258)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41061997413635254, max_val=0.5645580291748047)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2426], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-50.29859161376953, max_val=11.560476303100586)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0226], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8858323097229004, max_val=2.1025872230529785)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1240], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=31.44314956665039)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.834296226501465, max_val=1.2163833379745483)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): Identity()\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0919495820999146, max_val=0.9687157869338989)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1087861061096191, max_val=1.0919421911239624)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([151], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.42201805114746, max_val=12.651122093200684)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7721608877182007, max_val=0.7179678082466125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0918], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.278984069824219, max_val=12.137971878051758)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1995], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-25.44156837463379, max_val=24.005590438842773)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.1611], device='cuda:0'), zero_point=tensor([97], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-594.733642578125, max_val=976.3569946289062)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549964904785156)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.826887607574463, max_val=5.745780944824219)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.799777030944824, max_val=5.050131320953369)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0376], device='cuda:0'), zero_point=tensor([124], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.666715621948242, max_val=4.912500858306885)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6389373540878296, max_val=0.5619038343429565)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2267], device='cuda:0'), zero_point=tensor([226], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-51.21174621582031, max_val=6.595755100250244)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7547343969345093, max_val=1.132254958152771)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0584], device='cuda:0'), zero_point=tensor([3], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=14.731457710266113)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0380], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.847890853881836, max_val=1.818128228187561)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): Identity()\n",
            "        (10): Identity()\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.249541997909546, max_val=1.2171351909637451)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (key): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5546003580093384, max_val=1.382991909980774)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (value): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([180], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.16265296936035, max_val=8.026321411132812)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6351149082183838, max_val=0.6283748149871826)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (attention_scores_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0844], device='cuda:0'), zero_point=tensor([121], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.183473587036133, max_val=11.342721939086914)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.2967], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-36.96077346801758, max_val=37.82801055908203)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([20.1340], device='cuda:0'), zero_point=tensor([242], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4870.40966796875, max_val=263.76953125)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (context_layer_matmul): QATWrapper(\n",
            "                (forward_fn): QATMatMul()\n",
            "                (input_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0549681186676025)\n",
            "                    )\n",
            "                  )\n",
            "                  (1): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0465], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.77514123916626, max_val=5.931796073913574)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_quant_stubs): ModuleList(\n",
            "                  (0): QuantStub(\n",
            "                    (activation_post_process): FakeQuantize(\n",
            "                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0397], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.048597812652588, max_val=5.084214210510254)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (output_dequant_stubs): ModuleList(\n",
            "                  (0): DeQuantStub()\n",
            "                )\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): QuantWrapper(\n",
            "                (quant): QuantStub(\n",
            "                  (activation_post_process): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.8913421630859375, max_val=4.933834075927734)\n",
            "                  )\n",
            "                )\n",
            "                (dequant): DeQuantStub()\n",
            "                (module): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (weight_fake_quant): FakeQuantize(\n",
            "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7808341979980469, max_val=0.8295480608940125)\n",
            "                  )\n",
            "                  (activation_post_process): Identity()\n",
            "                )\n",
            "              )\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1086], device='cuda:0'), zero_point=tensor([207], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.42236328125, max_val=5.262651443481445)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=768, out_features=3072, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.842707633972168, max_val=0.7473668456077576)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): QuantWrapper(\n",
            "              (quant): QuantStub(\n",
            "                (activation_post_process): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0240], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997124254703522, max_val=5.961795806884766)\n",
            "                )\n",
            "              )\n",
            "              (dequant): DeQuantStub()\n",
            "              (module): Linear(\n",
            "                in_features=3072, out_features=768, bias=True\n",
            "                (weight_fake_quant): FakeQuantize(\n",
            "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.540349245071411, max_val=1.0925571918487549)\n",
            "                )\n",
            "                (activation_post_process): Identity()\n",
            "              )\n",
            "            )\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): QuantWrapper(\n",
            "        (quant): QuantStub(\n",
            "          (activation_post_process): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([146], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.598691463470459, max_val=4.8956828117370605)\n",
            "          )\n",
            "        )\n",
            "        (dequant): DeQuantStub()\n",
            "        (module): Linear(\n",
            "          in_features=768, out_features=768, bias=True\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0008], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09762699902057648, max_val=0.09738196432590485)\n",
            "          )\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "      )\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): QuantWrapper(\n",
            "    (quant): QuantStub(\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0083], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0535449981689453, max_val=1.0532469749450684)\n",
            "      )\n",
            "    )\n",
            "    (dequant): DeQuantStub()\n",
            "    (module): Linear(\n",
            "      in_features=768, out_features=6, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0006], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07281223684549332, max_val=0.07974009215831757)\n",
            "      )\n",
            "      (activation_post_process): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            " 93% 4200/4500 [42:52<03:10,  1.57it/s][INFO|trainer.py:571] 2022-11-17 16:21:24,445 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:21:24,449 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:21:24,449 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:21:24,449 >>   Batch size = 32\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:11,  5.24it/s]\u001b[A\n",
            "  5% 3/63 [00:00<00:16,  3.71it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:18,  3.18it/s]\u001b[A\n",
            "  8% 5/63 [00:01<00:19,  2.95it/s]\u001b[A\n",
            " 10% 6/63 [00:01<00:20,  2.82it/s]\u001b[A\n",
            " 11% 7/63 [00:02<00:20,  2.74it/s]\u001b[A\n",
            " 13% 8/63 [00:02<00:20,  2.69it/s]\u001b[A\n",
            " 14% 9/63 [00:03<00:20,  2.66it/s]\u001b[A\n",
            " 16% 10/63 [00:03<00:20,  2.64it/s]\u001b[A\n",
            " 17% 11/63 [00:03<00:19,  2.62it/s]\u001b[A\n",
            " 19% 12/63 [00:04<00:19,  2.61it/s]\u001b[A\n",
            " 21% 13/63 [00:04<00:19,  2.61it/s]\u001b[A\n",
            " 22% 14/63 [00:05<00:18,  2.61it/s]\u001b[A\n",
            " 24% 15/63 [00:05<00:18,  2.60it/s]\u001b[A\n",
            " 25% 16/63 [00:05<00:18,  2.60it/s]\u001b[A\n",
            " 27% 17/63 [00:06<00:17,  2.60it/s]\u001b[A\n",
            " 29% 18/63 [00:06<00:17,  2.59it/s]\u001b[A\n",
            " 30% 19/63 [00:06<00:16,  2.60it/s]\u001b[A\n",
            " 32% 20/63 [00:07<00:16,  2.60it/s]\u001b[A\n",
            " 33% 21/63 [00:07<00:16,  2.59it/s]\u001b[A\n",
            " 35% 22/63 [00:08<00:15,  2.59it/s]\u001b[A\n",
            " 37% 23/63 [00:08<00:15,  2.59it/s]\u001b[A\n",
            " 38% 24/63 [00:08<00:15,  2.60it/s]\u001b[A\n",
            " 40% 25/63 [00:09<00:14,  2.60it/s]\u001b[A\n",
            " 41% 26/63 [00:09<00:14,  2.59it/s]\u001b[A\n",
            " 43% 27/63 [00:10<00:13,  2.59it/s]\u001b[A\n",
            " 44% 28/63 [00:10<00:13,  2.60it/s]\u001b[A\n",
            " 46% 29/63 [00:10<00:13,  2.60it/s]\u001b[A\n",
            " 48% 30/63 [00:11<00:12,  2.56it/s]\u001b[A\n",
            " 49% 31/63 [00:11<00:12,  2.56it/s]\u001b[A\n",
            " 51% 32/63 [00:11<00:12,  2.58it/s]\u001b[A\n",
            " 52% 33/63 [00:12<00:11,  2.58it/s]\u001b[A\n",
            " 54% 34/63 [00:12<00:11,  2.58it/s]\u001b[A\n",
            " 56% 35/63 [00:13<00:10,  2.55it/s]\u001b[A\n",
            " 57% 36/63 [00:13<00:10,  2.56it/s]\u001b[A\n",
            " 59% 37/63 [00:13<00:10,  2.57it/s]\u001b[A\n",
            " 60% 38/63 [00:14<00:09,  2.55it/s]\u001b[A\n",
            " 62% 39/63 [00:14<00:09,  2.55it/s]\u001b[A\n",
            " 63% 40/63 [00:15<00:09,  2.55it/s]\u001b[A\n",
            " 65% 41/63 [00:15<00:08,  2.54it/s]\u001b[A\n",
            " 67% 42/63 [00:15<00:08,  2.56it/s]\u001b[A\n",
            " 68% 43/63 [00:16<00:07,  2.57it/s]\u001b[A\n",
            " 70% 44/63 [00:16<00:07,  2.58it/s]\u001b[A\n",
            " 71% 45/63 [00:17<00:06,  2.59it/s]\u001b[A\n",
            " 73% 46/63 [00:17<00:06,  2.58it/s]\u001b[A\n",
            " 75% 47/63 [00:17<00:06,  2.59it/s]\u001b[A\n",
            " 76% 48/63 [00:18<00:05,  2.60it/s]\u001b[A\n",
            " 78% 49/63 [00:18<00:05,  2.59it/s]\u001b[A\n",
            " 79% 50/63 [00:18<00:05,  2.60it/s]\u001b[A\n",
            " 81% 51/63 [00:19<00:04,  2.59it/s]\u001b[A\n",
            " 83% 52/63 [00:19<00:04,  2.60it/s]\u001b[A\n",
            " 84% 53/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 86% 54/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 87% 55/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 89% 56/63 [00:21<00:02,  2.60it/s]\u001b[A\n",
            " 90% 57/63 [00:21<00:02,  2.59it/s]\u001b[A\n",
            " 92% 58/63 [00:22<00:01,  2.59it/s]\u001b[A\n",
            " 94% 59/63 [00:22<00:01,  2.60it/s]\u001b[A\n",
            " 95% 60/63 [00:22<00:01,  2.59it/s]\u001b[A\n",
            " 97% 61/63 [00:23<00:00,  2.60it/s]\u001b[A\n",
            " 98% 62/63 [00:23<00:00,  2.60it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.13512611389160156, 'eval_accuracy': 0.9204999804496765, 'eval_runtime': 24.1884, 'eval_samples_per_second': 82.684, 'eval_steps_per_second': 2.605, 'epoch': 8.4}\n",
            " 93% 4200/4500 [43:16<03:10,  1.57it/s]\n",
            "100% 63/63 [00:23<00:00,  2.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2161] 2022-11-17 16:21:48,639 >> Saving model checkpoint to sparse_model/checkpoint-4200\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:21:48,640 >> Configuration saved in sparse_model/checkpoint-4200/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:21:49,369 >> Model weights saved in sparse_model/checkpoint-4200/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:21:49,370 >> tokenizer config file saved in sparse_model/checkpoint-4200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:21:49,370 >> Special tokens file saved in sparse_model/checkpoint-4200/special_tokens_map.json\n",
            "2022-11-17 16:21:49 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/checkpoint-4200/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/checkpoint-4200/recipe.yaml\n",
            "[INFO|trainer.py:2239] 2022-11-17 16:21:51,391 >> Deleting older checkpoint [sparse_model/checkpoint-3600] due to args.save_total_limit\n",
            " 98% 4400/4500 [45:26<01:04,  1.54it/s][INFO|trainer.py:571] 2022-11-17 16:23:58,695 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:23:58,701 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:23:58,701 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:23:58,701 >>   Batch size = 32\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:11,  5.21it/s]\u001b[A\n",
            "  5% 3/63 [00:00<00:16,  3.60it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:18,  3.14it/s]\u001b[A\n",
            "  8% 5/63 [00:01<00:19,  2.93it/s]\u001b[A\n",
            " 10% 6/63 [00:01<00:20,  2.81it/s]\u001b[A\n",
            " 11% 7/63 [00:02<00:20,  2.74it/s]\u001b[A\n",
            " 13% 8/63 [00:02<00:20,  2.70it/s]\u001b[A\n",
            " 14% 9/63 [00:03<00:20,  2.66it/s]\u001b[A\n",
            " 16% 10/63 [00:03<00:20,  2.64it/s]\u001b[A\n",
            " 17% 11/63 [00:03<00:19,  2.63it/s]\u001b[A\n",
            " 19% 12/63 [00:04<00:19,  2.63it/s]\u001b[A\n",
            " 21% 13/63 [00:04<00:19,  2.62it/s]\u001b[A\n",
            " 22% 14/63 [00:05<00:18,  2.60it/s]\u001b[A\n",
            " 24% 15/63 [00:05<00:18,  2.60it/s]\u001b[A\n",
            " 25% 16/63 [00:05<00:18,  2.60it/s]\u001b[A\n",
            " 27% 17/63 [00:06<00:17,  2.60it/s]\u001b[A\n",
            " 29% 18/63 [00:06<00:17,  2.60it/s]\u001b[A\n",
            " 30% 19/63 [00:06<00:16,  2.60it/s]\u001b[A\n",
            " 32% 20/63 [00:07<00:16,  2.60it/s]\u001b[A\n",
            " 33% 21/63 [00:07<00:16,  2.60it/s]\u001b[A\n",
            " 35% 22/63 [00:08<00:15,  2.60it/s]\u001b[A\n",
            " 37% 23/63 [00:08<00:15,  2.59it/s]\u001b[A\n",
            " 38% 24/63 [00:08<00:14,  2.60it/s]\u001b[A\n",
            " 40% 25/63 [00:09<00:14,  2.59it/s]\u001b[A\n",
            " 41% 26/63 [00:09<00:14,  2.60it/s]\u001b[A\n",
            " 43% 27/63 [00:10<00:13,  2.61it/s]\u001b[A\n",
            " 44% 28/63 [00:10<00:13,  2.62it/s]\u001b[A\n",
            " 46% 29/63 [00:10<00:12,  2.63it/s]\u001b[A\n",
            " 48% 30/63 [00:11<00:12,  2.62it/s]\u001b[A\n",
            " 49% 31/63 [00:11<00:12,  2.61it/s]\u001b[A\n",
            " 51% 32/63 [00:11<00:11,  2.61it/s]\u001b[A\n",
            " 52% 33/63 [00:12<00:11,  2.60it/s]\u001b[A\n",
            " 54% 34/63 [00:12<00:11,  2.61it/s]\u001b[A\n",
            " 56% 35/63 [00:13<00:10,  2.60it/s]\u001b[A\n",
            " 57% 36/63 [00:13<00:10,  2.60it/s]\u001b[A\n",
            " 59% 37/63 [00:13<00:10,  2.60it/s]\u001b[A\n",
            " 60% 38/63 [00:14<00:09,  2.60it/s]\u001b[A\n",
            " 62% 39/63 [00:14<00:09,  2.60it/s]\u001b[A\n",
            " 63% 40/63 [00:15<00:08,  2.60it/s]\u001b[A\n",
            " 65% 41/63 [00:15<00:08,  2.60it/s]\u001b[A\n",
            " 67% 42/63 [00:15<00:08,  2.60it/s]\u001b[A\n",
            " 68% 43/63 [00:16<00:07,  2.59it/s]\u001b[A\n",
            " 70% 44/63 [00:16<00:07,  2.60it/s]\u001b[A\n",
            " 71% 45/63 [00:16<00:06,  2.60it/s]\u001b[A\n",
            " 73% 46/63 [00:17<00:06,  2.60it/s]\u001b[A\n",
            " 75% 47/63 [00:17<00:06,  2.60it/s]\u001b[A\n",
            " 76% 48/63 [00:18<00:05,  2.60it/s]\u001b[A\n",
            " 78% 49/63 [00:18<00:05,  2.59it/s]\u001b[A\n",
            " 79% 50/63 [00:18<00:05,  2.60it/s]\u001b[A\n",
            " 81% 51/63 [00:19<00:04,  2.60it/s]\u001b[A\n",
            " 83% 52/63 [00:19<00:04,  2.59it/s]\u001b[A\n",
            " 84% 53/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 86% 54/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 87% 55/63 [00:20<00:03,  2.60it/s]\u001b[A\n",
            " 89% 56/63 [00:21<00:02,  2.59it/s]\u001b[A\n",
            " 90% 57/63 [00:21<00:02,  2.59it/s]\u001b[A\n",
            " 92% 58/63 [00:21<00:01,  2.60it/s]\u001b[A\n",
            " 94% 59/63 [00:22<00:01,  2.60it/s]\u001b[A\n",
            " 95% 60/63 [00:22<00:01,  2.59it/s]\u001b[A\n",
            " 97% 61/63 [00:23<00:00,  2.60it/s]\u001b[A\n",
            " 98% 62/63 [00:23<00:00,  2.60it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.13227687776088715, 'eval_accuracy': 0.9229999780654907, 'eval_runtime': 24.0959, 'eval_samples_per_second': 83.002, 'eval_steps_per_second': 2.615, 'epoch': 8.8}\n",
            " 98% 4400/4500 [45:50<01:04,  1.54it/s]\n",
            "100% 63/63 [00:23<00:00,  2.98it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2161] 2022-11-17 16:24:22,798 >> Saving model checkpoint to sparse_model/checkpoint-4400\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:24:22,800 >> Configuration saved in sparse_model/checkpoint-4400/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:24:23,504 >> Model weights saved in sparse_model/checkpoint-4400/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:24:23,504 >> tokenizer config file saved in sparse_model/checkpoint-4400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:24:23,505 >> Special tokens file saved in sparse_model/checkpoint-4400/special_tokens_map.json\n",
            "2022-11-17 16:24:23 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/checkpoint-4400/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/checkpoint-4400/recipe.yaml\n",
            "[INFO|trainer.py:2239] 2022-11-17 16:24:25,518 >> Deleting older checkpoint [sparse_model/checkpoint-3800] due to args.save_total_limit\n",
            "{'loss': 0.0994, 'learning_rate': 1.2666666666674112e-08, 'epoch': 9.0}\n",
            "100% 4500/4500 [46:57<00:00,  1.58it/s][INFO|trainer.py:1520] 2022-11-17 16:25:29,494 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2817.6742, 'train_samples_per_second': 51.106, 'train_steps_per_second': 1.597, 'train_loss': 0.3024353552924262, 'epoch': 9.0}\n",
            "100% 4500/4500 [46:57<00:00,  1.60it/s]\n",
            "2022-11-17 16:25:29 sparseml.transformers.sparsification.trainer INFO     Finalized SparseML recipe argument applied to the model\n",
            "INFO:sparseml.transformers.sparsification.trainer:Finalized SparseML recipe argument applied to the model\n",
            "2022-11-17 16:25:29 sparseml.transformers.sparsification.trainer INFO     Sparsification info for /root/.cache/sparsezoo/775fd536-75b0-4610-8cd7-1caab0e5a2f9/training: 66959622 total params. Of those there are 43061760 prunable params which have 78.89546548956662 avg sparsity.\n",
            "INFO:sparseml.transformers.sparsification.trainer:Sparsification info for /root/.cache/sparsezoo/775fd536-75b0-4610-8cd7-1caab0e5a2f9/training: 66959622 total params. Of those there are 43061760 prunable params which have 78.89546548956662 avg sparsity.\n",
            "2022-11-17 16:25:29 sparseml.transformers.sparsification.trainer INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66959622, \"sparse\": 33973776, \"sparsity_percent\": 50.737705777371325, \"prunable\": 43061760, \"prunable_sparse\": 33973776, \"prunable_sparsity_percent\": 78.89546548956662, \"quantizable\": 43104006, \"quantized\": 43104006, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": true}}}\n",
            "INFO:sparseml.transformers.sparsification.trainer:sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66959622, \"sparse\": 33973776, \"sparsity_percent\": 50.737705777371325, \"prunable\": 43061760, \"prunable_sparse\": 33973776, \"prunable_sparsity_percent\": 78.89546548956662, \"quantizable\": 43104006, \"quantized\": 43104006, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": true}}}\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.3024\n",
            "  train_runtime            = 0:46:57.67\n",
            "  train_samples            =      16000\n",
            "  train_samples_per_second =     51.106\n",
            "  train_steps_per_second   =      1.597\n",
            "[INFO|trainer.py:2161] 2022-11-17 16:25:29,582 >> Saving model checkpoint to sparse_model\n",
            "[INFO|configuration_utils.py:439] 2022-11-17 16:25:29,583 >> Configuration saved in sparse_model/config.json\n",
            "[INFO|modeling_utils.py:1084] 2022-11-17 16:25:30,293 >> Model weights saved in sparse_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-11-17 16:25:30,294 >> tokenizer config file saved in sparse_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-11-17 16:25:30,294 >> Special tokens file saved in sparse_model/special_tokens_map.json\n",
            "2022-11-17 16:25:30 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_model/recipe.yaml\n",
            "INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_model/recipe.yaml\n",
            "2022-11-17 16:25:32 sparseml.transformers.text_classification INFO     *** Evaluate ***\n",
            "INFO:sparseml.transformers.text_classification:*** Evaluate ***\n",
            "[INFO|trainer.py:571] 2022-11-17 16:25:32,298 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2411] 2022-11-17 16:25:32,304 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2413] 2022-11-17 16:25:32,304 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2416] 2022-11-17 16:25:32,304 >>   Batch size = 32\n",
            "100% 63/63 [00:08<00:00,  7.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =      0.925\n",
            "  eval_loss               =     0.1947\n",
            "  eval_runtime            = 0:00:08.73\n",
            "  eval_samples            =       2000\n",
            "  eval_samples_per_second =    228.834\n",
            "  eval_steps_per_second   =      7.208\n"
          ]
        }
      ],
      "source": [
        "!sparseml.transformers.train.text_classification \\\n",
        "  --output_dir sparse_model \\\n",
        "\t--model_name_or_path zoo:nlp/masked_language_modeling/obert-medium/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni \\\n",
        "\t--distill_teacher nateraw/bert-base-uncased-emotion \\\n",
        "\t--recipe zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni?recipe_type=transfer-text_classification \\\n",
        "  --dataset_name \"emotion\" \\\n",
        "  --recipe_args '{\"num_epochs\":9, \"init_lr\":0.000057}' \\\n",
        "\t--do_train \\\n",
        "\t--do_eval \\\n",
        "  --eval_steps 200 \\\n",
        "\t--max_seq_length 128 \\\n",
        "\t--evaluation_strategy steps \\\n",
        "\t--per_device_train_batch_size 32 \\\n",
        "\t--per_device_eval_batch_size 32 \\\n",
        "\t--preprocessing_num_workers 8 \\\n",
        "\t--fp16 \\\n",
        "\t--seed 42 \\\n",
        "\t--save_strategy steps \\\n",
        "\t--save_steps 200 \\\n",
        "\t--save_total_limit 3 \\\n",
        "\t--overwrite_output_dir \\\n",
        "\t--load_best_model_at_end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkq_oRzwHVT5"
      },
      "source": [
        "Now that we have a trained sparse model, we can export its PyTorch weights into ONNX format with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nF9eeA2HfPA",
        "outputId": "e3887655-e6b6-41ce-ddcf-9aacce5b0dc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "2022-11-17 16:25:55 sparseml.transformers.export INFO     Attempting onnx export for model at sparse_model for task text-classification\n",
            "INFO:sparseml.transformers.export:Attempting onnx export for model at sparse_model for task text-classification\n",
            "2022-11-17 16:25:55 sparseml.transformers.utils.model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied sparse_model\n",
            "WARNING:sparseml.transformers.utils.model:QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied sparse_model\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sparse_model and are newly initialized: ['encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'classifier.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-11-17 16:25:56 sparseml.transformers.utils.model INFO     Delayed load of model sparse_model detected. Will print out model information once SparseML recipes have loaded\n",
            "INFO:sparseml.transformers.utils.model:Delayed load of model sparse_model detected. Will print out model information once SparseML recipes have loaded\n",
            "2022-11-17 16:25:56 sparseml.transformers.export INFO     loaded model, config, and tokenizer from sparse_model\n",
            "INFO:sparseml.transformers.export:loaded model, config, and tokenizer from sparse_model\n",
            "2022-11-17 16:25:57 sparseml.transformers.sparsification.trainer INFO     Loaded 2 SparseML checkpoint recipe stage(s) from sparse_model/recipe.yaml to replicate model sparse state\n",
            "INFO:sparseml.transformers.sparsification.trainer:Loaded 2 SparseML checkpoint recipe stage(s) from sparse_model/recipe.yaml to replicate model sparse state\n",
            "2022-11-17 16:26:01 sparseml.transformers.sparsification.trainer INFO     Applied structure from 2 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n",
            "INFO:sparseml.transformers.sparsification.trainer:Applied structure from 2 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at sparse_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2022-11-17 16:26:02 sparseml.transformers.sparsification.trainer INFO     Reloaded 932 model params for SparseML Recipe from sparse_model\n",
            "INFO:sparseml.transformers.sparsification.trainer:Reloaded 932 model params for SparseML Recipe from sparse_model\n",
            "2022-11-17 16:26:02 sparseml.transformers.utils.model INFO     Loaded model from sparse_model with 66959622 total params. Of those there are 43061760 prunable params which have 78.89546548956662 avg sparsity.\n",
            "INFO:sparseml.transformers.utils.model:Loaded model from sparse_model with 66959622 total params. Of those there are 43061760 prunable params which have 78.89546548956662 avg sparsity.\n",
            "2022-11-17 16:26:02 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66959622, \"sparse\": 33973776, \"sparsity_percent\": 50.737705777371325, \"prunable\": 43061760, \"prunable_sparse\": 33973776, \"prunable_sparsity_percent\": 78.89546548956662, \"quantizable\": 43104006, \"quantized\": 43104006, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": true}}}\n",
            "INFO:sparseml.transformers.utils.model:sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66959622, \"sparse\": 33973776, \"sparsity_percent\": 50.737705777371325, \"prunable\": 43061760, \"prunable_sparse\": 33973776, \"prunable_sparsity_percent\": 78.89546548956662, \"quantizable\": 43104006, \"quantized\": 43104006, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.7999945878982544, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": true}}}\n",
            "2022-11-17 16:26:02 sparseml.transformers.sparsification.trainer INFO     Reloaded model state after SparseML recipe structure modifications from sparse_model\n",
            "INFO:sparseml.transformers.sparsification.trainer:Reloaded model state after SparseML recipe structure modifications from sparse_model\n",
            "2022-11-17 16:26:02 sparseml.transformers.export INFO     Applied a staged recipe with 2 stages to the model at sparse_model\n",
            "INFO:sparseml.transformers.export:Applied a staged recipe with 2 stages to the model at sparse_model\n",
            "2022-11-17 16:26:02 sparseml.transformers.export INFO     Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 128]', 'attention_mask': 'torch.int64: [1, 128]', 'token_type_ids': 'torch.int64: [1, 128]'}\n",
            "INFO:sparseml.transformers.export:Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 128]', 'attention_mask': 'torch.int64: [1, 128]', 'token_type_ids': 'torch.int64: [1, 128]'}\n",
            "/usr/local/lib/python3.7/dist-packages/sparseml/pytorch/sparsification/quantization/quantize_qat_export.py:334: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  tensor = torch.Tensor(array).to(torch.float32)\n",
            "2022-11-17 16:26:11 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 3 QAT embedding ops to UINT8\n",
            "INFO:sparseml.pytorch.sparsification.quantization.quantize_qat_export:Converted 3 QAT embedding ops to UINT8\n",
            "2022-11-17 16:26:11 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 12 quantizable MatMul ops to QLinearMatMul\n",
            "INFO:sparseml.pytorch.sparsification.quantization.quantize_qat_export:Converted 12 quantizable MatMul ops to QLinearMatMul\n",
            "2022-11-17 16:26:12 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 36 quantizable MatMul ops with weight and bias to MatMulInteger and Add\n",
            "INFO:sparseml.pytorch.sparsification.quantization.quantize_qat_export:Converted 36 quantizable MatMul ops with weight and bias to MatMulInteger and Add\n",
            "2022-11-17 16:26:12 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 2 quantizable Gemm ops with weight and bias to MatMulInteger and Add\n",
            "INFO:sparseml.pytorch.sparsification.quantization.quantize_qat_export:Converted 2 quantizable Gemm ops with weight and bias to MatMulInteger and Add\n",
            "2022-11-17 16:26:13 sparseml.transformers.export INFO     ONNX exported to sparse_model/model.onnx\n",
            "INFO:sparseml.transformers.export:ONNX exported to sparse_model/model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/sparseml.transformers.export_onnx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 391, in main\n",
            "    onnx_file_name=args.onnx_file_name,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 375, in export\n",
            "    training_directory=model_path, onnx_file_name=onnx_file_name\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 272, in create_deployment_folder\n",
            "    \"Expected to receive path to the training directory, \"\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "!sparseml.transformers.export_onnx --model_path sparse_model --task 'text_classification' --sequence_length 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M9SUhDGhvOW"
      },
      "source": [
        "Let's do the same to the dense model. First we'll download it to a directory named `dense_model`, and then we'll export it to ONNX:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hcvxn6GzLh_D"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
        "tokenizer.save_pretrained(\"/content/dense_model\")\n",
        "model.save_pretrained(\"/content/dense_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reCoVSJOhutS",
        "outputId": "d83a2415-5884-4e72-e812-59f1c0755714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "2022-11-17 16:26:29 sparseml.transformers.export INFO     Attempting onnx export for model at dense_model for task text-classification\n",
            "INFO:sparseml.transformers.export:Attempting onnx export for model at dense_model for task text-classification\n",
            "2022-11-17 16:26:31 sparseml.transformers.utils.model INFO     Loaded model from dense_model with 109486854 total params. Of those there are 85529088 prunable params which have 0.0 avg sparsity.\n",
            "INFO:sparseml.transformers.utils.model:Loaded model from dense_model with 109486854 total params. Of those there are 85529088 prunable params which have 0.0 avg sparsity.\n",
            "2022-11-17 16:26:33 sparseml.transformers.utils.model INFO     dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109486854, \"sparse\": 0, \"sparsity_percent\": 0.0, \"prunable\": 85529088, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85612806, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": false}}}\n",
            "INFO:sparseml.transformers.utils.model:dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109486854, \"sparse\": 0, \"sparsity_percent\": 0.0, \"prunable\": 85529088, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85612806, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 4608, \"sparsity\": 0.0, \"quantized\": false}}}\n",
            "2022-11-17 16:26:33 sparseml.transformers.export INFO     loaded model, config, and tokenizer from dense_model\n",
            "INFO:sparseml.transformers.export:loaded model, config, and tokenizer from dense_model\n",
            "2022-11-17 16:26:37 sparseml.transformers.export WARNING  No recipes were applied for dense_model, check to make sure recipe(s) are stored in the model_path\n",
            "WARNING:sparseml.transformers.export:No recipes were applied for dense_model, check to make sure recipe(s) are stored in the model_path\n",
            "2022-11-17 16:26:37 sparseml.transformers.export INFO     Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 128]', 'attention_mask': 'torch.int64: [1, 128]', 'token_type_ids': 'torch.int64: [1, 128]'}\n",
            "INFO:sparseml.transformers.export:Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 128]', 'attention_mask': 'torch.int64: [1, 128]', 'token_type_ids': 'torch.int64: [1, 128]'}\n",
            "[W shape_type_inference.cpp:419] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
            "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:758 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f9da1e60a22 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x2a9 (0x7f9de5a70199 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0x60 (0x7f9de5a72bf0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #3: <unknown function> + 0x1a03d12 (0x7f9de61e7d12 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #4: at::redispatch::index_select(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb4 (0x7f9de606a0d4 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #5: <unknown function> + 0x3013df1 (0x7f9de77f7df1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #6: <unknown function> + 0x3014245 (0x7f9de77f8245 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #7: at::index_select(at::Tensor const&, long, at::Tensor const&) + 0x14e (0x7f9de5e88ece in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b50 (0x7f9df8a4d1c0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #9: <unknown function> + 0xafd3e1 (0x7f9df8a8a3e1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0x906 (0x7f9df8a8f256 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #11: <unknown function> + 0xb05004 (0x7f9df8a92004 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #12: <unknown function> + 0xa7bc00 (0x7f9df8a08c00 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #13: <unknown function> + 0x500b88 (0x7f9df848db88 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #14: _PyMethodDef_RawFastCallKeywords + 0x264 (0x58f6e4 in /usr/bin/python3)\n",
            "frame #15: _PyEval_EvalFrameDefault + 0x4492 (0x5105e2 in /usr/bin/python3)\n",
            "frame #16: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #17: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #18: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #19: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #20: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #22: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #23: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #24: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #25: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #26: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #27: _PyEval_EvalFrameDefault + 0x41d5 (0x510325 in /usr/bin/python3)\n",
            "frame #28: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #29: _PyFunction_FastCallDict + 0x25a (0x4bad0a in /usr/bin/python3)\n",
            "frame #30: _PyEval_EvalFrameDefault + 0x203c (0x50e18c in /usr/bin/python3)\n",
            "frame #31: _PyEval_EvalCodeWithName + 0xbbe (0x5b575e in /usr/bin/python3)\n",
            "frame #32: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #33: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #34: _PyEval_EvalCodeWithName + 0xbbe (0x5b575e in /usr/bin/python3)\n",
            "frame #35: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #36: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #37: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #38: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #39: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #40: _PyFunction_FastCallKeywords + 0x187 (0x58fd37 in /usr/bin/python3)\n",
            "frame #41: _PyEval_EvalFrameDefault + 0x3ac (0x50c4fc in /usr/bin/python3)\n",
            "frame #42: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #43: PyEval_EvalCode + 0x23 (0x6005a3 in /usr/bin/python3)\n",
            "frame #44: /usr/bin/python3() [0x607796]\n",
            "frame #45: PyRun_FileExFlags + 0x9c (0x60785c in /usr/bin/python3)\n",
            "frame #46: PyRun_SimpleFileExFlags + 0x196 (0x60a436 in /usr/bin/python3)\n",
            "frame #47: /usr/bin/python3() [0x64db82]\n",
            "frame #48: _Py_UnixMain + 0x2e (0x64dd2e in /usr/bin/python3)\n",
            "frame #49: __libc_start_main + 0xe7 (0x7f9e14a21c87 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #50: _start + 0x2a (0x5b636a in /usr/bin/python3)\n",
            " (function ComputeConstantFolding)\n",
            "[W shape_type_inference.cpp:419] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
            "Exception raised from index_select_out_cpu_ at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:758 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f9da1e60a22 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x2a9 (0x7f9de5a70199 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0x60 (0x7f9de5a72bf0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #3: <unknown function> + 0x1a03d12 (0x7f9de61e7d12 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #4: at::redispatch::index_select(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xb4 (0x7f9de606a0d4 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #5: <unknown function> + 0x3013df1 (0x7f9de77f7df1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #6: <unknown function> + 0x3014245 (0x7f9de77f8245 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #7: at::index_select(at::Tensor const&, long, at::Tensor const&) + 0x14e (0x7f9de5e88ece in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b50 (0x7f9df8a4d1c0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #9: <unknown function> + 0xafd3e1 (0x7f9df8a8a3e1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::string, c10::IValue, std::less<std::string>, std::allocator<std::pair<std::string const, c10::IValue> > > const&, int) + 0x906 (0x7f9df8a8f256 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #11: <unknown function> + 0xb05004 (0x7f9df8a92004 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #12: <unknown function> + 0xa7bc00 (0x7f9df8a08c00 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #13: <unknown function> + 0x500b88 (0x7f9df848db88 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #14: _PyMethodDef_RawFastCallKeywords + 0x264 (0x58f6e4 in /usr/bin/python3)\n",
            "frame #15: _PyEval_EvalFrameDefault + 0x4492 (0x5105e2 in /usr/bin/python3)\n",
            "frame #16: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #17: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #18: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #19: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #20: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #22: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #23: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #24: _PyEval_EvalFrameDefault + 0x41d5 (0x510325 in /usr/bin/python3)\n",
            "frame #25: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #26: _PyFunction_FastCallDict + 0x25a (0x4bad0a in /usr/bin/python3)\n",
            "frame #27: _PyEval_EvalFrameDefault + 0x203c (0x50e18c in /usr/bin/python3)\n",
            "frame #28: _PyEval_EvalCodeWithName + 0xbbe (0x5b575e in /usr/bin/python3)\n",
            "frame #29: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #30: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #31: _PyEval_EvalCodeWithName + 0xbbe (0x5b575e in /usr/bin/python3)\n",
            "frame #32: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #33: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #34: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #35: _PyFunction_FastCallKeywords + 0x37e (0x58ff2e in /usr/bin/python3)\n",
            "frame #36: _PyEval_EvalFrameDefault + 0x1332 (0x50d482 in /usr/bin/python3)\n",
            "frame #37: _PyFunction_FastCallKeywords + 0x187 (0x58fd37 in /usr/bin/python3)\n",
            "frame #38: _PyEval_EvalFrameDefault + 0x3ac (0x50c4fc in /usr/bin/python3)\n",
            "frame #39: _PyEval_EvalCodeWithName + 0x346 (0x5b4ee6 in /usr/bin/python3)\n",
            "frame #40: PyEval_EvalCode + 0x23 (0x6005a3 in /usr/bin/python3)\n",
            "frame #41: /usr/bin/python3() [0x607796]\n",
            "frame #42: PyRun_FileExFlags + 0x9c (0x60785c in /usr/bin/python3)\n",
            "frame #43: PyRun_SimpleFileExFlags + 0x196 (0x60a436 in /usr/bin/python3)\n",
            "frame #44: /usr/bin/python3() [0x64db82]\n",
            "frame #45: _Py_UnixMain + 0x2e (0x64dd2e in /usr/bin/python3)\n",
            "frame #46: __libc_start_main + 0xe7 (0x7f9e14a21c87 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "frame #47: _start + 0x2a (0x5b636a in /usr/bin/python3)\n",
            " (function ComputeConstantFolding)\n",
            "2022-11-17 16:26:47 sparseml.transformers.export INFO     ONNX exported to dense_model/model.onnx\n",
            "INFO:sparseml.transformers.export:ONNX exported to dense_model/model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/sparseml.transformers.export_onnx\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 391, in main\n",
            "    onnx_file_name=args.onnx_file_name,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 375, in export\n",
            "    training_directory=model_path, onnx_file_name=onnx_file_name\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sparseml/transformers/export.py\", line 272, in create_deployment_folder\n",
            "    \"Expected to receive path to the training directory, \"\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "!sparseml.transformers.export_onnx --model_path dense_model --task 'text_classification' --sequence_length 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npvhmyj5Hfog"
      },
      "source": [
        "Let's now install [DeepSparse](https://github.com/neuralmagic/deepsparse) and benchmark these two models on the colab's single CPU and compare their speeds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOaKwCT9Hgj4",
        "outputId": "1fdef0a0-7e45-4a47-de35-f303e4885c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepsparse\n",
            "  Downloading deepsparse-1.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 39.0 MB 18.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (3.19.6)\n",
            "Requirement already satisfied: onnx<=1.12.0,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (1.10.1)\n",
            "Requirement already satisfied: pydantic>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (1.10.2)\n",
            "Requirement already satisfied: sparsezoo~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (1.2.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (4.64.1)\n",
            "Requirement already satisfied: click~=8.0.0 in /usr/local/lib/python3.7/dist-packages (from deepsparse) (8.0.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click~=8.0.0->deepsparse) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx<=1.12.0,>=1.5.0->deepsparse) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx<=1.12.0,>=1.5.0->deepsparse) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->deepsparse) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->deepsparse) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->deepsparse) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->deepsparse) (2.10)\n",
            "Requirement already satisfied: pyyaml>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from sparsezoo~=1.2.0->deepsparse) (6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click~=8.0.0->deepsparse) (3.10.0)\n",
            "Installing collected packages: deepsparse\n",
            "Successfully installed deepsparse-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install deepsparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9CpG1uVauQx",
        "outputId": "019c33b8-8d0a-4360-f3c0-f72a549ce7a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-17 16:26:58 deepsparse.benchmark.benchmark_model INFO     Thread pinning to cores enabled\n",
            "2022-11-17 16:26:58 deepsparse.benchmark.benchmark_model INFO     num_streams default value chosen of 1. This requires tuning and may be sub-optimal\n",
            "DeepSparse Engine, Copyright 2021-present / Neuralmagic, Inc. version: 1.2.0 COMMUNITY EDITION | (45d54d49) (release) (optimized) (system=avx2, binary=avx2)\n",
            "2022-11-17 16:27:30 deepsparse.benchmark.benchmark_model INFO     deepsparse.engine.Engine:\n",
            "\tonnx_file_path: dense_model/model.onnx\n",
            "\tbatch_size: 1\n",
            "\tnum_cores: 1\n",
            "\tnum_streams: 1\n",
            "\tscheduler: Scheduler.multi_stream\n",
            "\tcpu_avx_type: avx2\n",
            "\tcpu_vnni: False\n",
            "2022-11-17 16:27:30 deepsparse.utils.onnx INFO     Generating input 'input_ids', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:27:30 deepsparse.utils.onnx INFO     Generating input 'attention_mask', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:27:30 deepsparse.utils.onnx INFO     Generating input 'token_type_ids', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:27:30 deepsparse.benchmark.benchmark_model INFO     Starting 'multistream' performance measurements for 10 seconds\n",
            "Original Model Path: dense_model/model.onnx\n",
            "Batch Size: 1\n",
            "Scenario: async\n",
            "Throughput (items/sec): 2.4378\n",
            "Latency Mean (ms/batch): 410.1603\n",
            "Latency Median (ms/batch): 405.5149\n",
            "Latency Std (ms/batch): 11.5935\n",
            "Iterations: 25\n"
          ]
        }
      ],
      "source": [
        "!deepsparse.benchmark dense_model/model.onnx --batch_size 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu84qy9Tileq",
        "outputId": "a3e378fe-8241-497d-feb1-654867c5419c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-17 16:27:44 deepsparse.benchmark.benchmark_model INFO     Thread pinning to cores enabled\n",
            "2022-11-17 16:27:44 deepsparse.benchmark.benchmark_model INFO     num_streams default value chosen of 1. This requires tuning and may be sub-optimal\n",
            "DeepSparse Engine, Copyright 2021-present / Neuralmagic, Inc. version: 1.2.0 COMMUNITY EDITION | (45d54d49) (release) (optimized) (system=avx2, binary=avx2)\n",
            "2022-11-17 16:28:10 deepsparse.benchmark.benchmark_model INFO     deepsparse.engine.Engine:\n",
            "\tonnx_file_path: sparse_model/model.onnx\n",
            "\tbatch_size: 1\n",
            "\tnum_cores: 1\n",
            "\tnum_streams: 1\n",
            "\tscheduler: Scheduler.multi_stream\n",
            "\tcpu_avx_type: avx2\n",
            "\tcpu_vnni: False\n",
            "2022-11-17 16:28:10 deepsparse.utils.onnx INFO     Generating input 'input_ids', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:28:10 deepsparse.utils.onnx INFO     Generating input 'attention_mask', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:28:10 deepsparse.utils.onnx INFO     Generating input 'token_type_ids', type = int64, shape = [1, 128]\n",
            "2022-11-17 16:28:10 deepsparse.benchmark.benchmark_model INFO     Starting 'multistream' performance measurements for 10 seconds\n",
            "Original Model Path: sparse_model/model.onnx\n",
            "Batch Size: 1\n",
            "Scenario: async\n",
            "Throughput (items/sec): 18.3189\n",
            "Latency Mean (ms/batch): 54.5345\n",
            "Latency Median (ms/batch): 49.3620\n",
            "Latency Std (ms/batch): 11.9779\n",
            "Iterations: 184\n"
          ]
        }
      ],
      "source": [
        "!deepsparse.benchmark sparse_model/model.onnx --batch_size 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eojmQ3IyjqoG"
      },
      "source": [
        "Pretty incredible, the dense model gives us a latency of `410 ms` while the new sparse model gives us a latency of only `54 ms`, nearly an 8X speedup!! 🤯🤯🤯\n",
        "\n",
        "<br>\n",
        "\n",
        "For more resources, you can always give [SparseML](https://github.com/neuralmagic/sparseml) and [DeepSparse](https://github.com/neuralmagic/deepsparse) a ⭐, and let us know what you think on our [slack community channel](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "76154bc82c2616fabd4c1f760c8b3c8f69d8589c1172d5747525490253b05a58"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
