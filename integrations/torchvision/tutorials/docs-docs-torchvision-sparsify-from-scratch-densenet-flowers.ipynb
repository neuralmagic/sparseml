{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e081b7",
   "metadata": {},
   "source": [
    "# Sparsifying DenseNet121 from Scratch (Flower102)\n",
    "\n",
    "In this example, we will demonstrate how to sparsify an image classification model from scratch using SparseML's PyTorch integration. We train and prune [DenseNet121](https://pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html) on the downstream [Oxford Flower 102 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Flowers102.html#:~:text=Oxford%20102%20Flower%20is%20an,scale%2C%20pose%20and%20light%20variations) using the Global Magnitude Pruning algorithm. \n",
    "\n",
    "## Agenda\n",
    "\n",
    "There are a few steps:\n",
    "\n",
    " 1. Setup the dataset\n",
    " 2. Setup the PyTorch training loop\n",
    " 3. Train a dense version of DenseNet121\n",
    " 4. Run the GMP pruning algorithm on the dense model\n",
    " \n",
    "## Installation\n",
    "\n",
    "Install SparseML with `pip`:\n",
    "\n",
    "```\n",
    "pip install sparseml[torchvision]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad80edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sparseml\n",
    "import torchvision\n",
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "from sparseml.pytorch.utils import TensorBoardLogger, ModuleExporter, get_prunable_layers, tensor_sparsity\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a128dcc",
   "metadata": {},
   "source": [
    "## **Step 1: Setup Dataset**\n",
    "\n",
    "Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers were chosen to be flowers commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category, and several very similar categories.\n",
    "\n",
    "We use the standard PyTorch `datasets` and `dataloaders` to manage the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d180bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 102\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# imagenet transforms\n",
    "imagenet_transform = transforms.Compose([\n",
    "   transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=None),\n",
    "   transforms.CenterCrop(size=(224, 224)),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# datasets\n",
    "train_dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    transform=imagenet_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"val\",\n",
    "    transform=imagenet_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=16)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cca4a",
   "metadata": {},
   "source": [
    "## Step 2: Setup PyTorch Training Loop\n",
    "\n",
    "We will use this training loop below. This is standard PyTorch functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1b878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "def run_model_one_epoch(model, data_loader, criterion, device, train=False, optimizer=None):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # loop through batches\n",
    "    for step, (inputs, labels) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # compute loss, run backpropogation\n",
    "        outputs = model(inputs)  # model returns logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # run evaluation\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        total_correct += torch.sum(predictions == labels).item()\n",
    "        total_predictions += inputs.size(0)\n",
    "\n",
    "    # return loss and evaluation metric\n",
    "    loss = running_loss / (step + 1.0)\n",
    "    accuracy = total_correct / total_predictions\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385497a",
   "metadata": {},
   "source": [
    "## **Step 3: Train DenseNet121 on Flowers102**\n",
    "\n",
    "First, we will train a dense version of DenseNet121 on the Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d554578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download pre-trained model, setup classification head\n",
    "model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.DEFAULT)\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_LABELS)\n",
    "model.to(device)\n",
    "\n",
    "# setup loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=8e-3) # lr will be override by sparseml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb400dc3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Next, we will use SparseML's recipes to set the hyperparameters of training loop. In this case, we will use the following recipe:\n",
    "\n",
    "```yaml\n",
    "# Epoch and Learning-Rate variables\n",
    "num_epochs: 10.0\n",
    "init_lr: 0.0005\n",
    "\n",
    "training_modifiers:\n",
    "  - !EpochRangeModifier\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(num_epochs)\n",
    "\n",
    "  - !LearningRateFunctionModifier\n",
    "    final_lr: 0.0\n",
    "    init_lr: eval(init_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(num_epochs)\n",
    "```\n",
    "\n",
    "As you can see, the recipe includes an `!EpochRangeModifier` and a `!LearningRateFunctionModifier`. These modifiers simply set the number of epochs to train for and the learning rate schedule. As a result, the final model will be dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6a53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_recipe_path = \"./recipes/densenet-flowers-dense-recipe.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8653c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "# Epoch and Learning-Rate variables\r\n",
      "num_epochs: 10.0\r\n",
      "init_lr: 0.0005\r\n",
      "\r\n",
      "training_modifiers:\r\n",
      "  - !EpochRangeModifier\r\n",
      "    start_epoch: 0.0\r\n",
      "    end_epoch: eval(num_epochs)\r\n",
      "\r\n",
      "  - !LearningRateFunctionModifier\r\n",
      "    final_lr: 0.0\r\n",
      "    init_lr: eval(init_lr)\r\n",
      "    lr_func: cosine\r\n",
      "    start_epoch: 0.0\r\n",
      "    end_epoch: eval(num_epochs)\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./recipes/densenet-flowers-dense-recipe.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31aed1b",
   "metadata": {},
   "source": [
    "Next, we use SparseML's `ScheduledModifierManager` to parse and apply the recipe. The `manager.modify` function modifies and wraps the `model` and `optimizer` with the instructions from the recipe. You can use the `model` and `optimizer` just like standard PyTorch objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1749e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ScheduledModifierManager and Optimizer wrapper\n",
    "manager = ScheduledModifierManager.from_yaml(dense_recipe_path)\n",
    "optimizer = manager.modify(model, optimizer, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67c917",
   "metadata": {},
   "source": [
    "Kick off the transfer learning loop. Our run reached ~92.5% validation accuracy after 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00d175b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1f4f772bc540b0b3725dddfa1b0f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/10\n",
      "Training Loss: 3.6993999630212784\n",
      "Top 1 Acc: 0.2823529411764706\n",
      "\n",
      "Running Validation Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4865f146cabb4142b9eb8b7a6a7c59f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 1/10\n",
      "Val Loss: 2.32899521663785\n",
      "Top 1 Acc: 0.6235294117647059\n",
      "\n",
      "Running Training Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d973db1bca35479c8e5482dc2a8b45e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/10\n",
      "Training Loss: 1.3755246959626675\n",
      "Top 1 Acc: 0.8823529411764706\n",
      "\n",
      "Running Validation Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a896b5e2fe64e0f9353c2c0bb3ab2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 2/10\n",
      "Val Loss: 1.0808461774140596\n",
      "Top 1 Acc: 0.85\n",
      "\n",
      "Running Training Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd04f003b34476696b88bdcf3ffb8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/10\n",
      "Training Loss: 0.4065576898865402\n",
      "Top 1 Acc: 0.9882352941176471\n",
      "\n",
      "Running Validation Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dffd5826f924b0eb7bca6039f439d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 3/10\n",
      "Val Loss: 0.7663819249719381\n",
      "Top 1 Acc: 0.8823529411764706\n",
      "\n",
      "Running Training Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec7462e075e446185eddbd2537748e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/10\n",
      "Training Loss: 0.13117457507178187\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee0722460fb4916ae605a705703df8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 4/10\n",
      "Val Loss: 0.5690533611923456\n",
      "Top 1 Acc: 0.9127450980392157\n",
      "\n",
      "Running Training Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520064cc7f9a486391e6c8cf34dcc46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5/10\n",
      "Training Loss: 0.05944129719864577\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b6421ee84d48d7ab9411a7fea5cae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 5/10\n",
      "Val Loss: 0.4922205451875925\n",
      "Top 1 Acc: 0.9147058823529411\n",
      "\n",
      "Running Training Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3c98c1b78b406e8e61ef9b3c27ad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6/10\n",
      "Training Loss: 0.04032939835451543\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7d5076d9fe4f91aaf6290e20ed73c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 6/10\n",
      "Val Loss: 0.47672522626817226\n",
      "Top 1 Acc: 0.9205882352941176\n",
      "\n",
      "Running Training Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43f39481dd94010b65d8a4bfa2c82bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7/10\n",
      "Training Loss: 0.03349028015509248\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31258f33c4b439b838a723a0b9e7787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 7/10\n",
      "Val Loss: 0.4664669381454587\n",
      "Top 1 Acc: 0.9176470588235294\n",
      "\n",
      "Running Training Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45670df899134e2688a5fae7778bb392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8/10\n",
      "Training Loss: 0.03036679228534922\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3a457626624ce587eac6b63928b80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 8/10\n",
      "Val Loss: 0.45540827978402376\n",
      "Top 1 Acc: 0.9264705882352942\n",
      "\n",
      "Running Training Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a396321f97e4010aa079b7a2954d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9/10\n",
      "Training Loss: 0.028963472577743232\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51950b565767410f8ef9cb052fcb6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 9/10\n",
      "Val Loss: 0.4513796488754451\n",
      "Top 1 Acc: 0.9235294117647059\n",
      "\n",
      "Running Training Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1085b12cebf24a2a8e7f69e3d13c9991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10/10\n",
      "Training Loss: 0.027131778770126402\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb6b899572c49dab5cd0aed15ede593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 10/10\n",
      "Val Loss: 0.45484112948179245\n",
      "Top 1 Acc: 0.9245098039215687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for epoch in range(manager.max_epochs):\n",
    "    # run training loop\n",
    "    epoch_name = f\"{epoch + 1}/{manager.max_epochs}\"\n",
    "    \n",
    "    print(f\"Running Training Epoch {epoch_name}\")\n",
    "    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)\n",
    "    print(f\"Training Epoch: {epoch_name}\\nTraining Loss: {train_loss}\\nTop 1 Acc: {train_acc}\\n\")\n",
    "\n",
    "    # run validation loop\n",
    "    print(f\"Running Validation Epoch {epoch_name}\")\n",
    "    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Epoch: {epoch_name}\\nVal Loss: {val_loss}\\nTop 1 Acc: {val_acc}\\n\")\n",
    "\n",
    "# clean up\n",
    "manager.finalize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3d14e",
   "metadata": {},
   "source": [
    "Export the model in case we want to reload in the future, so we do not have to rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"dense_model\"\n",
    "exporter = ModuleExporter(model, output_dir=save_dir)\n",
    "exporter.export_pytorch(name=\"densenet-121-dense-flowers.pth\")\n",
    "exporter.export_onnx(torch.randn(1, 3, 224, 224), name=\"dense-model.onnx\", convert_qat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4532442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d6309",
   "metadata": {},
   "source": [
    "## Step 4: Prune The Model\n",
    "\n",
    "With a model trained on Flowers, we are now ready to apply the GMP algorithm to prune the model. The GMP algorithm is an interative pruning algorithm. At the end of each epoch, we identify the lowest magnitude weights (those closest to 0) and remove them from the network starting from an initial level of sparsity until a final level of sparsity. The remaining nonzero weights are then fine-tuned onto training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8447d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the trained model from Part 3\n",
    "checkpoint = torch.load(\"./dense_model/training/densenet-121-dense-flowers.pth\")\n",
    "model = torchvision.models.densenet121()\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_LABELS)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# setup loss function and optimizer, LR will be overriden by sparseml\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=8e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05d884",
   "metadata": {},
   "source": [
    "Next, we need to create a SparseML recipe which includes the GMP algorithm. The `!GlobalMagnitudePruningModifier` modifier instructs SparseML to apply the GMP algorithm at a global level (pruning the lowest magnitude weights across all layers).\n",
    "\n",
    "Firstly, we need to decide identify which parameters of the model to apply the GMP algorithm to. We can use the `get_prunable_layers` function to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d367905a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0\n",
      "features.denseblock1.denselayer1.conv1\n",
      "features.denseblock1.denselayer1.conv2\n",
      "features.denseblock1.denselayer2.conv1\n",
      "features.denseblock1.denselayer2.conv2\n",
      "features.denseblock1.denselayer3.conv1\n",
      "features.denseblock1.denselayer3.conv2\n",
      "features.denseblock1.denselayer4.conv1\n",
      "features.denseblock1.denselayer4.conv2\n",
      "features.denseblock1.denselayer5.conv1\n",
      "features.denseblock1.denselayer5.conv2\n",
      "features.denseblock1.denselayer6.conv1\n",
      "features.denseblock1.denselayer6.conv2\n",
      "features.transition1.conv\n",
      "features.denseblock2.denselayer1.conv1\n",
      "features.denseblock2.denselayer1.conv2\n",
      "features.denseblock2.denselayer2.conv1\n",
      "features.denseblock2.denselayer2.conv2\n",
      "features.denseblock2.denselayer3.conv1\n",
      "features.denseblock2.denselayer3.conv2\n",
      "features.denseblock2.denselayer4.conv1\n",
      "features.denseblock2.denselayer4.conv2\n",
      "features.denseblock2.denselayer5.conv1\n",
      "features.denseblock2.denselayer5.conv2\n",
      "features.denseblock2.denselayer6.conv1\n",
      "features.denseblock2.denselayer6.conv2\n",
      "features.denseblock2.denselayer7.conv1\n",
      "features.denseblock2.denselayer7.conv2\n",
      "features.denseblock2.denselayer8.conv1\n",
      "features.denseblock2.denselayer8.conv2\n",
      "features.denseblock2.denselayer9.conv1\n",
      "features.denseblock2.denselayer9.conv2\n",
      "features.denseblock2.denselayer10.conv1\n",
      "features.denseblock2.denselayer10.conv2\n",
      "features.denseblock2.denselayer11.conv1\n",
      "features.denseblock2.denselayer11.conv2\n",
      "features.denseblock2.denselayer12.conv1\n",
      "features.denseblock2.denselayer12.conv2\n",
      "features.transition2.conv\n",
      "features.denseblock3.denselayer1.conv1\n",
      "features.denseblock3.denselayer1.conv2\n",
      "features.denseblock3.denselayer2.conv1\n",
      "features.denseblock3.denselayer2.conv2\n",
      "features.denseblock3.denselayer3.conv1\n",
      "features.denseblock3.denselayer3.conv2\n",
      "features.denseblock3.denselayer4.conv1\n",
      "features.denseblock3.denselayer4.conv2\n",
      "features.denseblock3.denselayer5.conv1\n",
      "features.denseblock3.denselayer5.conv2\n",
      "features.denseblock3.denselayer6.conv1\n",
      "features.denseblock3.denselayer6.conv2\n",
      "features.denseblock3.denselayer7.conv1\n",
      "features.denseblock3.denselayer7.conv2\n",
      "features.denseblock3.denselayer8.conv1\n",
      "features.denseblock3.denselayer8.conv2\n",
      "features.denseblock3.denselayer9.conv1\n",
      "features.denseblock3.denselayer9.conv2\n",
      "features.denseblock3.denselayer10.conv1\n",
      "features.denseblock3.denselayer10.conv2\n",
      "features.denseblock3.denselayer11.conv1\n",
      "features.denseblock3.denselayer11.conv2\n",
      "features.denseblock3.denselayer12.conv1\n",
      "features.denseblock3.denselayer12.conv2\n",
      "features.denseblock3.denselayer13.conv1\n",
      "features.denseblock3.denselayer13.conv2\n",
      "features.denseblock3.denselayer14.conv1\n",
      "features.denseblock3.denselayer14.conv2\n",
      "features.denseblock3.denselayer15.conv1\n",
      "features.denseblock3.denselayer15.conv2\n",
      "features.denseblock3.denselayer16.conv1\n",
      "features.denseblock3.denselayer16.conv2\n",
      "features.denseblock3.denselayer17.conv1\n",
      "features.denseblock3.denselayer17.conv2\n",
      "features.denseblock3.denselayer18.conv1\n",
      "features.denseblock3.denselayer18.conv2\n",
      "features.denseblock3.denselayer19.conv1\n",
      "features.denseblock3.denselayer19.conv2\n",
      "features.denseblock3.denselayer20.conv1\n",
      "features.denseblock3.denselayer20.conv2\n",
      "features.denseblock3.denselayer21.conv1\n",
      "features.denseblock3.denselayer21.conv2\n",
      "features.denseblock3.denselayer22.conv1\n",
      "features.denseblock3.denselayer22.conv2\n",
      "features.denseblock3.denselayer23.conv1\n",
      "features.denseblock3.denselayer23.conv2\n",
      "features.denseblock3.denselayer24.conv1\n",
      "features.denseblock3.denselayer24.conv2\n",
      "features.transition3.conv\n",
      "features.denseblock4.denselayer1.conv1\n",
      "features.denseblock4.denselayer1.conv2\n",
      "features.denseblock4.denselayer2.conv1\n",
      "features.denseblock4.denselayer2.conv2\n",
      "features.denseblock4.denselayer3.conv1\n",
      "features.denseblock4.denselayer3.conv2\n",
      "features.denseblock4.denselayer4.conv1\n",
      "features.denseblock4.denselayer4.conv2\n",
      "features.denseblock4.denselayer5.conv1\n",
      "features.denseblock4.denselayer5.conv2\n",
      "features.denseblock4.denselayer6.conv1\n",
      "features.denseblock4.denselayer6.conv2\n",
      "features.denseblock4.denselayer7.conv1\n",
      "features.denseblock4.denselayer7.conv2\n",
      "features.denseblock4.denselayer8.conv1\n",
      "features.denseblock4.denselayer8.conv2\n",
      "features.denseblock4.denselayer9.conv1\n",
      "features.denseblock4.denselayer9.conv2\n",
      "features.denseblock4.denselayer10.conv1\n",
      "features.denseblock4.denselayer10.conv2\n",
      "features.denseblock4.denselayer11.conv1\n",
      "features.denseblock4.denselayer11.conv2\n",
      "features.denseblock4.denselayer12.conv1\n",
      "features.denseblock4.denselayer12.conv2\n",
      "features.denseblock4.denselayer13.conv1\n",
      "features.denseblock4.denselayer13.conv2\n",
      "features.denseblock4.denselayer14.conv1\n",
      "features.denseblock4.denselayer14.conv2\n",
      "features.denseblock4.denselayer15.conv1\n",
      "features.denseblock4.denselayer15.conv2\n",
      "features.denseblock4.denselayer16.conv1\n",
      "features.denseblock4.denselayer16.conv2\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "# print parameters\n",
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fa136",
   "metadata": {},
   "source": [
    "We will apply pruning to each of the `convs` and exclude the `classifier` (which is the final projection head). Fortunately, SparseML allows us to pass regexes to identify layers in the network, so we can use the following list to identify the relevant layers for pruning:\n",
    "\n",
    "    - 'features.conv0.weight'\n",
    "    - 're:features.denseblock1.*.conv1.weight'\n",
    "    - 're:features.denseblock1.*.conv2.weight'\n",
    "    - 're:features.transition1.conv.weight'\n",
    "    - 're:features.denseblock2.*.conv1.weight'\n",
    "    - 're:features.denseblock2.*.conv2.weight'\n",
    "    - 're:features.transition2.conv.weight'\n",
    "    - 're:features.denseblock3.*.conv1.weight'\n",
    "    - 're:features.denseblock3.*.conv2.weight'\n",
    "    - 're:features.transition3.conv.weight'\n",
    "    - 're:features.denseblock4.*.conv1.weight'\n",
    "    - 're:features.denseblock4.*.conv2.weight'\n",
    "\n",
    "Here is what the recipe looks like:\n",
    "\n",
    "```yaml\n",
    "# Epoch and Learning-Rate variables\n",
    "num_epochs: 13.0\n",
    "pruning_epochs: 10.0\n",
    "init_lr: 0.00025\n",
    "final_lr: 0.0001\n",
    "inter_func: cubic\n",
    "mask_type: unstructured\n",
    "\n",
    "training_modifiers:\n",
    "  - !EpochRangeModifier\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(num_epochs)\n",
    "\n",
    "  - !LearningRateFunctionModifier\n",
    "    final_lr: eval(final_lr)\n",
    "    init_lr: eval(init_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(pruning_epochs)\n",
    "    \n",
    "  - !LearningRateFunctionModifier\n",
    "    final_lr: eval(final_lr)\n",
    "    init_lr: eval(init_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: eval(pruning_epochs)\n",
    "    end_epoch: eval(num_epochs)\n",
    "\n",
    "# Pruning\n",
    "pruning_modifiers:\n",
    "  - !GlobalMagnitudePruningModifier\n",
    "    init_sparsity: 0.05\n",
    "    final_sparsity: 0.85\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(pruning_epochs)\n",
    "    update_frequency: 0.5\n",
    "    params: \n",
    "        - 'features.conv0.weight'\n",
    "        - 're:features.denseblock1.*.conv1.weight'\n",
    "        - 're:features.denseblock1.*.conv2.weight'\n",
    "        - 're:features.transition1.conv.weight'\n",
    "        - 're:features.denseblock2.*.conv1.weight'\n",
    "        - 're:features.denseblock2.*.conv2.weight'\n",
    "        - 're:features.transition2.conv.weight'\n",
    "        - 're:features.denseblock3.*.conv1.weight'\n",
    "        - 're:features.denseblock3.*.conv2.weight'\n",
    "        - 're:features.transition3.conv.weight'\n",
    "        - 're:features.denseblock4.*.conv1.weight'\n",
    "        - 're:features.denseblock4.*.conv2.weight'\n",
    "    leave_enabled: True\n",
    "    inter_func: eval(inter_func)\n",
    "    mask_type: eval(mask_type)\n",
    "```\n",
    "\n",
    "This recipe specifies that we will run the GMP algorithm for the first 10 epochs. We start at an `init_sparsity` level of 5% and gradually increase sparsity to a `final_sparsity` level of 85% following a `cubic` curve. The pruning is applied in an `unstructured` manner, meaning that any weight can be pruned.\n",
    "\n",
    "Over the final 3 epochs, we will fine-tune the 85% pruned model further. Since we set `leave_enabled=True` the sparsity level will be maintained as the fine-tuning occurs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8518b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_recipe_path = \"./recipes/densenet-flowers-pruning-recipe.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17fc31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat ./recipes/densenet-flowers-pruning-recipe.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68646bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ScheduledModifierManager and Optimizer wrapper\n",
    "manager = ScheduledModifierManager.from_yaml(pruning_recipe_path)\n",
    "logger = TensorBoardLogger(log_path=\"./tensorboard_outputs\")\n",
    "optimizer = manager.modify(model, optimizer, loggers=[logger], steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a5909",
   "metadata": {},
   "source": [
    "Next, kick off the GMP training loop. \n",
    "\n",
    "As you can see, we use the wrapped `optimizer` and `model` in the same way as above. SparseML parsed the recipe and updated the `optimizer` with the logic of GMP algorithm from the recipe. This allows you to just the `optimizer` and `model` as usual, with all of the pruning-related logic specified by the declarative recipe interface.\n",
    "\n",
    "Our 85% sparsified model reaches ~91.5% validation accuracy (vs ~92.5% for the dense model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "601c8c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training Epoch 1/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181013fb564f45828d9cce72589b3846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/13\n",
      "Training Loss: 0.043860748992301524\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 1/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cdead2769d4377b6cf53fc91028e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 1/13\n",
      "Val Loss: 0.49546412169001997\n",
      "Top 1 Acc: 0.8950980392156863\n",
      "\n",
      "Running Training Epoch 2/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e647b8b6e413284659d33dbb5bf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/13\n",
      "Training Loss: 0.031713149510324\n",
      "Top 1 Acc: 0.9980392156862745\n",
      "\n",
      "Running Validation Epoch 2/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8af1f0eb7514408a36fad527f4119c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 2/13\n",
      "Val Loss: 0.380035838810727\n",
      "Top 1 Acc: 0.9117647058823529\n",
      "\n",
      "Running Training Epoch 3/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadd8b75f6a3468487921687fb7a1902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/13\n",
      "Training Loss: 0.016048584191594273\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 3/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23ad27123a84b9ab3e9d37bcfd94fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 3/13\n",
      "Val Loss: 0.3518236926756799\n",
      "Top 1 Acc: 0.9156862745098039\n",
      "\n",
      "Running Training Epoch 4/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5bbfdb2df24c67b036c84f67258727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/13\n",
      "Training Loss: 0.013376500370213762\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 4/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80d92282c594060bc65c655b0db0184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 4/13\n",
      "Val Loss: 0.37410336383618414\n",
      "Top 1 Acc: 0.9098039215686274\n",
      "\n",
      "Running Training Epoch 5/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4352b279fc0546458ce3e0717f41b051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5/13\n",
      "Training Loss: 0.01488300270284526\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 5/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e7239ec24f4a5cbaf9d38b942358b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 5/13\n",
      "Val Loss: 0.3517874537501484\n",
      "Top 1 Acc: 0.9156862745098039\n",
      "\n",
      "Running Training Epoch 6/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1448047bbf7f41fdbf84106aff303104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6/13\n",
      "Training Loss: 0.020687898708274588\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 6/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efe9031bf4b4ebbae2d992dc3266230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 6/13\n",
      "Val Loss: 0.38692406262271106\n",
      "Top 1 Acc: 0.9137254901960784\n",
      "\n",
      "Running Training Epoch 7/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27c0cbd9d6e4e02b44043050c58c2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7/13\n",
      "Training Loss: 0.029207701765699312\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 7/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1752d47a0c348999b1ab92624ebca01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 7/13\n",
      "Val Loss: 0.4347131696995348\n",
      "Top 1 Acc: 0.8990196078431373\n",
      "\n",
      "Running Training Epoch 8/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e764a1922b0740488b5ceb97a636f5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8/13\n",
      "Training Loss: 0.05598480207845569\n",
      "Top 1 Acc: 0.9950980392156863\n",
      "\n",
      "Running Validation Epoch 8/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28f85a917a94204b50687e4397313ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 8/13\n",
      "Val Loss: 0.5028206340502948\n",
      "Top 1 Acc: 0.8813725490196078\n",
      "\n",
      "Running Training Epoch 9/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f340b8a413e417ca9bad5a3201dd55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9/13\n",
      "Training Loss: 0.030482763570034876\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 9/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd02f0a64301403bb37da14323dd4284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 9/13\n",
      "Val Loss: 0.4592747155111283\n",
      "Top 1 Acc: 0.8941176470588236\n",
      "\n",
      "Running Training Epoch 10/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddef4c5bcba426784e011fc5a8659e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10/13\n",
      "Training Loss: 0.017036149831255898\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 10/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8d775853324429b9f7061478101253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 10/13\n",
      "Val Loss: 0.40747271745931357\n",
      "Top 1 Acc: 0.9019607843137255\n",
      "\n",
      "Running Training Epoch 11/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce09a16874e4e8c94deb40dea6bdb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11/13\n",
      "Training Loss: 0.008050348522374406\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 11/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8834eae2a924e6d8f6aa9c713f2cd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 11/13\n",
      "Val Loss: 0.36801655124872923\n",
      "Top 1 Acc: 0.9068627450980392\n",
      "\n",
      "Running Training Epoch 12/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1343cce0b7d9417ca5cf9b788e5bbcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12/13\n",
      "Training Loss: 0.005260431091301143\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 12/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5952900528a6494e9025afcce97fcef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 12/13\n",
      "Val Loss: 0.3571971161291003\n",
      "Top 1 Acc: 0.9117647058823529\n",
      "\n",
      "Running Training Epoch 13/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cb82ea02634c619c57a823e1d2872e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13/13\n",
      "Training Loss: 0.004477048831176944\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 13/13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81030fbb34a04181ae3cb25acc470275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 13/13\n",
      "Val Loss: 0.3466116476338357\n",
      "Top 1 Acc: 0.9137254901960784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run GMP algorithm\n",
    "epoch = 0\n",
    "for epoch in range(manager.max_epochs):\n",
    "    # run training loop\n",
    "    epoch_name = f\"{epoch + 1}/{manager.max_epochs}\"\n",
    "    \n",
    "    print(f\"Running Training Epoch {epoch_name}\")\n",
    "    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)\n",
    "    print(f\"Training Epoch: {epoch_name}\\nTraining Loss: {train_loss}\\nTop 1 Acc: {train_acc}\\n\")\n",
    "\n",
    "    # run validation loop\n",
    "    print(f\"Running Validation Epoch {epoch_name}\")\n",
    "    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Epoch: {epoch_name}\\nVal Loss: {val_loss}\\nTop 1 Acc: {val_acc}\\n\")\n",
    "    \n",
    "    logger.log_scalar(\"Metrics/Loss (Train)\", train_loss, epoch)\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Train)\", train_acc, epoch)\n",
    "    logger.log_scalar(\"Metrics/Loss (Validation)\", val_loss, epoch)\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Validation)\", val_acc, epoch)\n",
    "\n",
    "manager.finalize(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3be34fe5",
   "metadata": {},
   "source": [
    "Here is a sample of the TensorBoard output, showing the validation accuracy, a particular layer's sparsity level, and the learning rate over time.\n",
    "\n",
    "![tensorboard output](./images/densenet-flowers-tensorboard-output.png)\n",
    "\n",
    "We can print layer-by-layer sparsity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df5afa94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.weight: 0.4184\n",
      "features.denseblock1.denselayer1.conv1.weight: 0.6761\n",
      "features.denseblock1.denselayer1.conv2.weight: 0.7671\n",
      "features.denseblock1.denselayer2.conv1.weight: 0.6769\n",
      "features.denseblock1.denselayer2.conv2.weight: 0.8130\n",
      "features.denseblock1.denselayer3.conv1.weight: 0.6814\n",
      "features.denseblock1.denselayer3.conv2.weight: 0.7528\n",
      "features.denseblock1.denselayer4.conv1.weight: 0.7571\n",
      "features.denseblock1.denselayer4.conv2.weight: 0.7544\n",
      "features.denseblock1.denselayer5.conv1.weight: 0.8184\n",
      "features.denseblock1.denselayer5.conv2.weight: 0.8309\n",
      "features.denseblock1.denselayer6.conv1.weight: 0.7914\n",
      "features.denseblock1.denselayer6.conv2.weight: 0.7735\n",
      "features.transition1.conv.weight: 0.6437\n",
      "features.denseblock2.denselayer1.conv1.weight: 0.9088\n",
      "features.denseblock2.denselayer1.conv2.weight: 0.8509\n",
      "features.denseblock2.denselayer2.conv1.weight: 0.8362\n",
      "features.denseblock2.denselayer2.conv2.weight: 0.7972\n",
      "features.denseblock2.denselayer3.conv1.weight: 0.8374\n",
      "features.denseblock2.denselayer3.conv2.weight: 0.7750\n",
      "features.denseblock2.denselayer4.conv1.weight: 0.8064\n",
      "features.denseblock2.denselayer4.conv2.weight: 0.8011\n",
      "features.denseblock2.denselayer5.conv1.weight: 0.8513\n",
      "features.denseblock2.denselayer5.conv2.weight: 0.7810\n",
      "features.denseblock2.denselayer6.conv1.weight: 0.8767\n",
      "features.denseblock2.denselayer6.conv2.weight: 0.8048\n",
      "features.denseblock2.denselayer7.conv1.weight: 0.8297\n",
      "features.denseblock2.denselayer7.conv2.weight: 0.8145\n",
      "features.denseblock2.denselayer8.conv1.weight: 0.8360\n",
      "features.denseblock2.denselayer8.conv2.weight: 0.7907\n",
      "features.denseblock2.denselayer9.conv1.weight: 0.8567\n",
      "features.denseblock2.denselayer9.conv2.weight: 0.8235\n",
      "features.denseblock2.denselayer10.conv1.weight: 0.8154\n",
      "features.denseblock2.denselayer10.conv2.weight: 0.8226\n",
      "features.denseblock2.denselayer11.conv1.weight: 0.8305\n",
      "features.denseblock2.denselayer11.conv2.weight: 0.8266\n",
      "features.denseblock2.denselayer12.conv1.weight: 0.8147\n",
      "features.denseblock2.denselayer12.conv2.weight: 0.8448\n",
      "features.transition2.conv.weight: 0.6985\n",
      "features.denseblock3.denselayer1.conv1.weight: 0.8672\n",
      "features.denseblock3.denselayer1.conv2.weight: 0.8512\n",
      "features.denseblock3.denselayer2.conv1.weight: 0.9014\n",
      "features.denseblock3.denselayer2.conv2.weight: 0.8561\n",
      "features.denseblock3.denselayer3.conv1.weight: 0.8684\n",
      "features.denseblock3.denselayer3.conv2.weight: 0.8519\n",
      "features.denseblock3.denselayer4.conv1.weight: 0.8734\n",
      "features.denseblock3.denselayer4.conv2.weight: 0.8226\n",
      "features.denseblock3.denselayer5.conv1.weight: 0.8974\n",
      "features.denseblock3.denselayer5.conv2.weight: 0.8180\n",
      "features.denseblock3.denselayer6.conv1.weight: 0.8764\n",
      "features.denseblock3.denselayer6.conv2.weight: 0.8219\n",
      "features.denseblock3.denselayer7.conv1.weight: 0.9095\n",
      "features.denseblock3.denselayer7.conv2.weight: 0.8101\n",
      "features.denseblock3.denselayer8.conv1.weight: 0.8529\n",
      "features.denseblock3.denselayer8.conv2.weight: 0.8216\n",
      "features.denseblock3.denselayer9.conv1.weight: 0.8591\n",
      "features.denseblock3.denselayer9.conv2.weight: 0.8137\n",
      "features.denseblock3.denselayer10.conv1.weight: 0.9050\n",
      "features.denseblock3.denselayer10.conv2.weight: 0.8202\n",
      "features.denseblock3.denselayer11.conv1.weight: 0.8952\n",
      "features.denseblock3.denselayer11.conv2.weight: 0.8025\n",
      "features.denseblock3.denselayer12.conv1.weight: 0.8779\n",
      "features.denseblock3.denselayer12.conv2.weight: 0.8349\n",
      "features.denseblock3.denselayer13.conv1.weight: 0.9029\n",
      "features.denseblock3.denselayer13.conv2.weight: 0.7903\n",
      "features.denseblock3.denselayer14.conv1.weight: 0.9133\n",
      "features.denseblock3.denselayer14.conv2.weight: 0.8175\n",
      "features.denseblock3.denselayer15.conv1.weight: 0.8790\n",
      "features.denseblock3.denselayer15.conv2.weight: 0.8279\n",
      "features.denseblock3.denselayer16.conv1.weight: 0.8979\n",
      "features.denseblock3.denselayer16.conv2.weight: 0.8660\n",
      "features.denseblock3.denselayer17.conv1.weight: 0.8712\n",
      "features.denseblock3.denselayer17.conv2.weight: 0.8479\n",
      "features.denseblock3.denselayer18.conv1.weight: 0.8885\n",
      "features.denseblock3.denselayer18.conv2.weight: 0.8439\n",
      "features.denseblock3.denselayer19.conv1.weight: 0.8771\n",
      "features.denseblock3.denselayer19.conv2.weight: 0.8424\n",
      "features.denseblock3.denselayer20.conv1.weight: 0.8775\n",
      "features.denseblock3.denselayer20.conv2.weight: 0.8493\n",
      "features.denseblock3.denselayer21.conv1.weight: 0.8743\n",
      "features.denseblock3.denselayer21.conv2.weight: 0.8407\n",
      "features.denseblock3.denselayer22.conv1.weight: 0.8802\n",
      "features.denseblock3.denselayer22.conv2.weight: 0.8169\n",
      "features.denseblock3.denselayer23.conv1.weight: 0.8757\n",
      "features.denseblock3.denselayer23.conv2.weight: 0.8595\n",
      "features.denseblock3.denselayer24.conv1.weight: 0.8835\n",
      "features.denseblock3.denselayer24.conv2.weight: 0.8561\n",
      "features.transition3.conv.weight: 0.7817\n",
      "features.denseblock4.denselayer1.conv1.weight: 0.8013\n",
      "features.denseblock4.denselayer1.conv2.weight: 0.8734\n",
      "features.denseblock4.denselayer2.conv1.weight: 0.8007\n",
      "features.denseblock4.denselayer2.conv2.weight: 0.9233\n",
      "features.denseblock4.denselayer3.conv1.weight: 0.8141\n",
      "features.denseblock4.denselayer3.conv2.weight: 0.8888\n",
      "features.denseblock4.denselayer4.conv1.weight: 0.8402\n",
      "features.denseblock4.denselayer4.conv2.weight: 0.9181\n",
      "features.denseblock4.denselayer5.conv1.weight: 0.8396\n",
      "features.denseblock4.denselayer5.conv2.weight: 0.9465\n",
      "features.denseblock4.denselayer6.conv1.weight: 0.8353\n",
      "features.denseblock4.denselayer6.conv2.weight: 0.9475\n",
      "features.denseblock4.denselayer7.conv1.weight: 0.8559\n",
      "features.denseblock4.denselayer7.conv2.weight: 0.9485\n",
      "features.denseblock4.denselayer8.conv1.weight: 0.8596\n",
      "features.denseblock4.denselayer8.conv2.weight: 0.9471\n",
      "features.denseblock4.denselayer9.conv1.weight: 0.8662\n",
      "features.denseblock4.denselayer9.conv2.weight: 0.9583\n",
      "features.denseblock4.denselayer10.conv1.weight: 0.8692\n",
      "features.denseblock4.denselayer10.conv2.weight: 0.9559\n",
      "features.denseblock4.denselayer11.conv1.weight: 0.8692\n",
      "features.denseblock4.denselayer11.conv2.weight: 0.9565\n",
      "features.denseblock4.denselayer12.conv1.weight: 0.8767\n",
      "features.denseblock4.denselayer12.conv2.weight: 0.9562\n",
      "features.denseblock4.denselayer13.conv1.weight: 0.8804\n",
      "features.denseblock4.denselayer13.conv2.weight: 0.9558\n",
      "features.denseblock4.denselayer14.conv1.weight: 0.8836\n",
      "features.denseblock4.denselayer14.conv2.weight: 0.9550\n",
      "features.denseblock4.denselayer15.conv1.weight: 0.8865\n",
      "features.denseblock4.denselayer15.conv2.weight: 0.9575\n",
      "features.denseblock4.denselayer16.conv1.weight: 0.8806\n",
      "features.denseblock4.denselayer16.conv2.weight: 0.9506\n",
      "classifier.weight: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}.weight: {tensor_sparsity(layer.weight).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fed85b",
   "metadata": {},
   "source": [
    "Finally, export your model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "417ac105",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"experiment-0\"\n",
    "exporter = ModuleExporter(model, output_dir=save_dir)\n",
    "exporter.export_pytorch(name=\"densenet-121-sparse-flowers.pth\")\n",
    "exporter.export_onnx(torch.randn(1, 3, 224, 224), name=\"sparse-model.onnx\", convert_qat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c96232",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "The resulting model is is 85% sparse and achieves validation accuracy of ~91.5% (vs the unoptimized dense model at ~92.5%) without much hyperparameter search.\n",
    "\n",
    "Key hyperparameter experiments you may want to run include:\n",
    "- Learning rate\n",
    "- Learning rate schedule\n",
    "- Sparsity level\n",
    "- Number of pruning epochs\n",
    "\n",
    "\n",
    "DeepSparse supports speedup from pruning and quantization. To reach maximum performance, check out our examples of quantizing a model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
