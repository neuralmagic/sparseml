---
title: "Creating with SparseML Recipe Template"
metaTitle: "Creating Sparsification Recipes with SparseML"
metaDescription: "Creating Sparsification Recipes with SparseML"
index: 1000 
---

# Creating Sparsification Recipes with SparseML Recipe Template

This page explains how to create recipes using the SparseML Recipe Template local API. 

The SparseML Recipe Template API is a simple API that outtakes in an optional path to a dense Pytorch model as input and returns a [Sparsification recipe](https://docs.neuralmagic.com/user-guide/recipes) for that given model in the form of a `recipe.md` file. If no model is supplied, SparseML Recipe Template will generate a default Sparsification Recipe for a generic model. The Sparsification recipe that is outputted by a call to the SopaseML Recipe Template API can then be applied to a model in [Pytorch, Keras, or TensorFlow V1 training pipelines](https://docs.neuralmagic.com/user-guide/recipes/enabling) to train asparse model, all with just a few lines of code. 

SparseML Recipe Template is built for users who may be inclined to create their recipes by hand to enable more
fine-grained control or add custom modifiers when sparsifying a new model from scratch with a single API command. 


## Overview

SparseML Recipe Template takes in the path to an optional Pytorch model that the you wish to generate a Sparsification recipe for; if none is provided, Recipe Template will generate a generic recipe based on the defaults and designated parameters provided. You may supply a path to a torchscript model (ie *.pt) that can be loaded with `torch.jit.load`. If you are using the python API, you may also supply an already constructed pytorch Module.


You have many different options for configuring the Recipe Template API to generate a sparse model to your choosing leveraging techniques like pruning and quantization. Recipe Template has many available arguements which allows you to fully customize the recipe that gets created. A handful of them are required to help designate the techniques used in generating a Sparsification recipe.

To run the SparseML Recipe Template API, you use the following command: 

```bash
  SparseML.recipe_template(model[optional]) --args[optional]
```

Arguments can be presented to designate different aspects of the recipe such as whether it should include pruning and if so, what algorithm should be used, quantization and if so, what algorithm should be used, as well as the learning rate type. 

### Base Arguments:
```bash
--pruning true|false(default)|ALGORITHM
ALGORITHM: global magnitude pruning, gradual magnitude pruning â€“ sparsity per layer, ACDC, Movement, MFAC, OBC
--quantization true|false(default)|TARGET_HARDWARE(default to QAT algorithm ex: vnni, tensorrt)
--lr TYPE
TYPE: constant, stepped, cyclic, exponential, linear
```

### Creating the Recipe Template
Once SparseML.recipe_template is run, a base recipe template will be created with the necessary top level variables based on the compression methods specified (ie pruning, quantization). The repo will maintain versions for pruning, qat, and pruning+qat.  These templates will either be python strings with `{variable}` placeholders left in for on the fly formatting, or functions that take in the desired variables and return the constructed template.

*Notes on template construction:*

- For QAT only, a constant pruning modifier with `__ALL_PRUNABLE__` will be added.
- If a pytorch model is supplied, pruning params will be selected using this helper function. If time is allowed the following filters among others should be added:
  - Ignoring depthwise layers in mobilenet-like models (ie convs with groups==num_channels)
  - Ignoring the first and last layers for pruning
  - [stretch] Setting a minimum number of parameters a layer should have to be considered prunable in the recipe
- Variables and values in the template recipe should then be updated according to user input and the standards set in the comments above along with acceptance criteria.
- Different pruning modifiers may have different constructor argos - will need to account for this when swapping the algorithm

### Enabling the Recipe Template

The output will be a `recipe.md` file, with information added based on intended usage.

The yaml frontmatter of the markdown recipe will be the generated recipe template.  In the markdown section of the recipe, the user will be provided instructions on how to plug this recipe into a Manager and run training aware (and one-shot for testing) using SparseML.

Additionally, a printout of the settable recipe variables and instructions on how to override in the Manager API will be provided. [STRETCH] parse documentation on each variable from comments in the recipe template, or [MINIMAL] direct the user to read in the yaml section above.


## Additional Features:
- Sparse transfer mode (use constant pruning modifier instead of pruning modifier)
- Convenience functions to replace manager from_yaml (ie manager.sparsification(...))
- For LR modifiers:
  - When pruning, cycle through init_lr and final_lr
  - When finetuning, cycle through init_lr and final_lr
  - When quantizing, hold constant at final_lr


### Additional Recipe Template Modifyers

In addition to the base arguments listed above, you can also fine tune specific parameters at a more detialed level such as Epoch, LR, Pruning, QAT, and Modifyers to guide the recipe you want created for your model. To view the exaustive list of arguments available, run the following help command:








